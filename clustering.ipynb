{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AQzYGMVl4-1n"
   },
   "source": [
    "# Text Mining -  Group Project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qRnI9EF74-1t"
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TTwyDa3M4-1v"
   },
   "source": [
    "# Preparing the Data\n",
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "id": "x0DzGcra4-1x"
   },
   "outputs": [],
   "source": [
    "import gensim\n",
    "import hdbscan\n",
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import re\n",
    "import string\n",
    "import timeit\n",
    "import umap\n",
    "\n",
    "from itertools import chain\n",
    "from collections import Counter\n",
    "from sklearn import metrics\n",
    "from nltk.tokenize import regexp_tokenize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the Data\n",
    "\n",
    "The Reddit dataset can be retrieved from [here](https://www.kaggle.com/datasets/rootuser/worldnews-on-reddit) and the Irish News dataset can be retrieved from [here](https://www.kaggle.com/datasets/therohk/ireland-historical-news). In order to run the code, the datasets have to be manually downloaded. In the following code it is assumed that the datasets are stored in a directory named \"\"\"data\"\"\" which is located within the main repository. If the data is stored elsewhere, please adapt the path where indicated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reddit News"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "WZ2OPwtS4-10"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reddit_file_clmns: Index(['time_created', 'date_created', 'up_votes', 'down_votes', 'title',\n",
      "       'over_18', 'author', 'subreddit'],\n",
      "      dtype='object')\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Headline</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Scores killed in Pakistan clashes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Japan resumes refuelling mission</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>US presses Egypt on Gaza border</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Jump-start economy: Give health care to all</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Council of Europe bashes EU&amp;UN terror blacklist</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          Headline\n",
       "0                Scores killed in Pakistan clashes\n",
       "1                 Japan resumes refuelling mission\n",
       "2                  US presses Egypt on Gaza border\n",
       "3     Jump-start economy: Give health care to all \n",
       "4  Council of Europe bashes EU&UN terror blacklist"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reddit_path = \"data/reddit_worldnews.csv\" #Change path if necessary\n",
    "reddit_file = pd.read_csv(reddit_path, encoding=\"utf-8\", encoding_errors=\"ignore\")\n",
    "print(\"reddit_file_clmns:\", reddit_file.columns)\n",
    "reddit = pd.DataFrame(reddit_file[\"title\"]).rename(columns={\"title\":\"Headline\"})\n",
    "reddit.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Irland News"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {
    "id": "WZ2OPwtS4-10",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ireland_file_clmns: Index(['publish_date', 'headline_category', 'headline_text'], dtype='object')\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Headline</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>747865</th>\n",
       "      <td>Egypt moves to close Gaza border breach</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>747866</th>\n",
       "      <td>Almost two-thirds of voters undecided on EU tr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>747867</th>\n",
       "      <td>Jacob Fruitfield factory to lay off 220 staff</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>747868</th>\n",
       "      <td>Bono says rich world failing anti-poverty camp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>747869</th>\n",
       "      <td>Government; NRA announce €1.68bn roads plan</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 Headline\n",
       "747865            Egypt moves to close Gaza border breach\n",
       "747866  Almost two-thirds of voters undecided on EU tr...\n",
       "747867      Jacob Fruitfield factory to lay off 220 staff\n",
       "747868  Bono says rich world failing anti-poverty camp...\n",
       "747869        Government; NRA announce €1.68bn roads plan"
      ]
     },
     "execution_count": 208,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ireland_path = \"data/ireland-news-headlines.csv\" #Change path if necessary\n",
    "ireland_file = pd.read_csv(ireland_path, encoding=\"utf-8\", encoding_errors=\"ignore\")\n",
    "print(\"ireland_file_clmns:\", ireland_file.columns)\n",
    "ireland_filtered_date = ireland_file[(ireland_file[\"publish_date\"] >= 20080125)& \n",
    "                                     (ireland_file[\"publish_date\"] <= 20161122)]\n",
    "ireland = pd.DataFrame(ireland_filtered_date[\"headline_text\"]).rename(columns={\"headline_text\":\"Headline\"})\n",
    "ireland.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-processing\n",
    "Preprocessing includes tokenizing the data, removing stopwords and punctuation ans well as lemmatizing the tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pre-processing \n",
    "# Regular expression used for tokenization\n",
    "pattern = r'''(?x)    \n",
    "(?:[A-Z]\\.)+          \n",
    "|\\w+(?:-\\w+)*         \n",
    "|\\$?\\d+(?:\\.\\d+)?%?   \n",
    "|\\.\\.\\.               \n",
    "|[][.,;\"\\'?():-_`]  \n",
    "'''\n",
    "\n",
    "# Lemmatizer used \n",
    "lemmatizer = nltk.WordNetLemmatizer()\n",
    "\n",
    "def preprocessing(df):\n",
    "    \"\"\"Input: dataframe\n",
    "       Output: preprocessed dataframe\"\"\"\n",
    "    \n",
    "    # Reduce amount of data for quicker training purposes\n",
    "    # Headline = df[\"Headline\"].head(100)\n",
    "        \n",
    "    # Get the stopwords and punctuation\n",
    "    stopwords = nltk.corpus.stopwords.words('english')\n",
    "    punct = list(string.punctuation)\n",
    "    \n",
    "    # Initialize tokenized list of headlines\n",
    "    # Get list of headlines\n",
    "    headlns_lst = df[\"Headline\"].to_list()\n",
    "    \n",
    "    tokenized_lines = []\n",
    "    for headln in headlns_lst:\n",
    "        line = str(headln).strip().lower()\n",
    "        line = regexp_tokenize(line, pattern)\n",
    "        line = [tok for tok in line if tok not in stopwords and tok not in punct and tok.isalpha() and len(tok)>2]\n",
    "        tokenized_lines.append(line)\n",
    "    \n",
    "    # Initialize lemmatized list of headlines\n",
    "    pp_df = pd.DataFrame(columns = [\"Headline\"])\n",
    "    \n",
    "    lemmatized_lines = [[lemmatizer.lemmatize(token) for token in headln] for headln in tokenized_lines]\n",
    "    line_df = pd.DataFrame({\"Headline\": lemmatized_lines})\n",
    "    pp_df = pp_df.append(line_df, ignore_index=True)\n",
    " \n",
    "    return pp_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reddit News"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.07059840800002348\n"
     ]
    }
   ],
   "source": [
    "#Pre-processing the Reddit dataset\n",
    "reddit_pp = preprocessing(reddit)\n",
    "t = timeit.timeit(lambda:reddit_pp)\n",
    "print(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Headline</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[score, killed, pakistan, clash]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[japan, resume, refuelling, mission]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[press, egypt, gaza, border]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[economy, give, health, care]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[council, europe, bash, terror, blacklist]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     Headline\n",
       "0            [score, killed, pakistan, clash]\n",
       "1        [japan, resume, refuelling, mission]\n",
       "2                [press, egypt, gaza, border]\n",
       "3               [economy, give, health, care]\n",
       "4  [council, europe, bash, terror, blacklist]"
      ]
     },
     "execution_count": 270,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reddit_pp.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "509236"
      ]
     },
     "execution_count": 276,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(reddit_pp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ireland News"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.07471210899529979\n"
     ]
    }
   ],
   "source": [
    "# Pre-processing the Irish Times dataset\n",
    "ireland_pp = preprocessing(ireland)\n",
    "t = timeit.timeit(lambda:ireland_pp)\n",
    "print(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Headline</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[egypt, move, close, gaza, border, breach]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[almost, voter, undecided, treaty]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[jacob, fruitfield, factory, lay, staff]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[bono, say, rich, world, failing, campaign]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[government, nra, announce, road, plan]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      Headline\n",
       "0   [egypt, move, close, gaza, border, breach]\n",
       "1           [almost, voter, undecided, treaty]\n",
       "2     [jacob, fruitfield, factory, lay, staff]\n",
       "3  [bono, say, rich, world, failing, campaign]\n",
       "4      [government, nra, announce, road, plan]"
      ]
     },
     "execution_count": 272,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ireland_pp.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "598837"
      ]
     },
     "execution_count": 273,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ireland_pp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YwLrjSBV4-18"
   },
   "source": [
    "# Sentence Embeddings\n",
    "\n",
    "We are sentence embeddings by averaging the pre-trained word embeddings from the GoogleNews vectors. These embeddings can be found at https://drive.google.com/file/d/0B7XkCwpI5KDYNlNUTTlSS21pQmM/edit?resourcekey=0-wjGZdNAUop6WykTtMip30g. Download the dataset to the directory ```TMproject/data``` as with the previous files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the GoogleNews embeddings, change path id necessary\n",
    "model = gensim.models.KeyedVectors.load_word2vec_format('data/GoogleNews-vectors-negative300.bin.gz', binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_vector(word2vec_model, doc):\n",
    "    \"\"\"Calculate the mean vector according to a word2vec model for one document/headline\"\"\"\n",
    "    # remove out-of-vocabulary words\n",
    "    doc = [word for word in doc if word in word2vec_model]\n",
    "    if len(doc) >= 1:\n",
    "        return sum(word2vec_model[doc])/len(doc)\n",
    "    else:\n",
    "        return []\n",
    "\n",
    "def doc_embeddings(dataset):\n",
    "    \"\"\"Calculate the mean vector for all documents in a dataset.\n",
    "    Outputs a dataframe with document and 300 dim vector representation of it.\"\"\"\n",
    "    embeddings_df = pd.DataFrame()\n",
    "\n",
    "    for doc in dataset[\"Headline\"]:\n",
    "        vec = mean_vector(model, doc)\n",
    "        if len(vec) > 0:\n",
    "            vec_df = pd.Series(vec)\n",
    "            doc_df = pd.Series([doc]).append(vec_df, ignore_index = True)\n",
    "            embeddings_df = embeddings_df.append(doc_df, ignore_index=True) \n",
    "\n",
    "    return embeddings_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the above defined functions we created headline embeddings for each headline in the two datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embedding the Reddit headlines\n",
    "reddit_embed = doc_embeddings(reddit_pp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>291</th>\n",
       "      <th>292</th>\n",
       "      <th>293</th>\n",
       "      <th>294</th>\n",
       "      <th>295</th>\n",
       "      <th>296</th>\n",
       "      <th>297</th>\n",
       "      <th>298</th>\n",
       "      <th>299</th>\n",
       "      <th>300</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[score, killed, pakistan, clash]</td>\n",
       "      <td>-0.016129</td>\n",
       "      <td>0.087769</td>\n",
       "      <td>0.174744</td>\n",
       "      <td>0.032501</td>\n",
       "      <td>0.062805</td>\n",
       "      <td>0.018555</td>\n",
       "      <td>0.054245</td>\n",
       "      <td>-0.231934</td>\n",
       "      <td>-0.019043</td>\n",
       "      <td>...</td>\n",
       "      <td>0.011230</td>\n",
       "      <td>-0.065674</td>\n",
       "      <td>-0.117554</td>\n",
       "      <td>0.085938</td>\n",
       "      <td>-0.078674</td>\n",
       "      <td>0.006287</td>\n",
       "      <td>-0.067261</td>\n",
       "      <td>-0.108887</td>\n",
       "      <td>0.037109</td>\n",
       "      <td>0.077209</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[japan, resume, refuelling, mission]</td>\n",
       "      <td>-0.159668</td>\n",
       "      <td>0.098999</td>\n",
       "      <td>0.081258</td>\n",
       "      <td>0.101440</td>\n",
       "      <td>-0.027832</td>\n",
       "      <td>0.044149</td>\n",
       "      <td>-0.012207</td>\n",
       "      <td>-0.201497</td>\n",
       "      <td>0.269694</td>\n",
       "      <td>...</td>\n",
       "      <td>0.005534</td>\n",
       "      <td>0.073608</td>\n",
       "      <td>-0.049805</td>\n",
       "      <td>0.068034</td>\n",
       "      <td>-0.031067</td>\n",
       "      <td>-0.114746</td>\n",
       "      <td>-0.174072</td>\n",
       "      <td>0.117676</td>\n",
       "      <td>0.026042</td>\n",
       "      <td>-0.007975</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[press, egypt, gaza, border]</td>\n",
       "      <td>-0.023438</td>\n",
       "      <td>0.138458</td>\n",
       "      <td>-0.059265</td>\n",
       "      <td>0.034424</td>\n",
       "      <td>-0.024048</td>\n",
       "      <td>-0.052673</td>\n",
       "      <td>-0.092682</td>\n",
       "      <td>-0.093994</td>\n",
       "      <td>0.023010</td>\n",
       "      <td>...</td>\n",
       "      <td>0.017792</td>\n",
       "      <td>0.044189</td>\n",
       "      <td>-0.009369</td>\n",
       "      <td>-0.029266</td>\n",
       "      <td>0.012695</td>\n",
       "      <td>-0.024078</td>\n",
       "      <td>-0.188660</td>\n",
       "      <td>-0.022461</td>\n",
       "      <td>0.017151</td>\n",
       "      <td>0.098053</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[economy, give, health, care]</td>\n",
       "      <td>-0.005249</td>\n",
       "      <td>0.042480</td>\n",
       "      <td>0.000671</td>\n",
       "      <td>0.056671</td>\n",
       "      <td>0.016022</td>\n",
       "      <td>-0.035461</td>\n",
       "      <td>0.098572</td>\n",
       "      <td>-0.119873</td>\n",
       "      <td>0.085754</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.073151</td>\n",
       "      <td>0.017151</td>\n",
       "      <td>0.017548</td>\n",
       "      <td>-0.055221</td>\n",
       "      <td>-0.057190</td>\n",
       "      <td>0.163560</td>\n",
       "      <td>-0.052979</td>\n",
       "      <td>0.052917</td>\n",
       "      <td>0.126587</td>\n",
       "      <td>-0.110962</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[council, europe, bash, terror, blacklist]</td>\n",
       "      <td>-0.027258</td>\n",
       "      <td>0.040814</td>\n",
       "      <td>0.126221</td>\n",
       "      <td>0.196533</td>\n",
       "      <td>-0.120996</td>\n",
       "      <td>-0.073096</td>\n",
       "      <td>-0.180078</td>\n",
       "      <td>-0.112744</td>\n",
       "      <td>0.060791</td>\n",
       "      <td>...</td>\n",
       "      <td>0.098340</td>\n",
       "      <td>0.071582</td>\n",
       "      <td>0.000366</td>\n",
       "      <td>0.184985</td>\n",
       "      <td>-0.114307</td>\n",
       "      <td>-0.017651</td>\n",
       "      <td>-0.054053</td>\n",
       "      <td>-0.059961</td>\n",
       "      <td>0.040894</td>\n",
       "      <td>-0.090448</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 301 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          0         1         2         3    \\\n",
       "0            [score, killed, pakistan, clash] -0.016129  0.087769  0.174744   \n",
       "1        [japan, resume, refuelling, mission] -0.159668  0.098999  0.081258   \n",
       "2                [press, egypt, gaza, border] -0.023438  0.138458 -0.059265   \n",
       "3               [economy, give, health, care] -0.005249  0.042480  0.000671   \n",
       "4  [council, europe, bash, terror, blacklist] -0.027258  0.040814  0.126221   \n",
       "\n",
       "        4         5         6         7         8         9    ...       291  \\\n",
       "0  0.032501  0.062805  0.018555  0.054245 -0.231934 -0.019043  ...  0.011230   \n",
       "1  0.101440 -0.027832  0.044149 -0.012207 -0.201497  0.269694  ...  0.005534   \n",
       "2  0.034424 -0.024048 -0.052673 -0.092682 -0.093994  0.023010  ...  0.017792   \n",
       "3  0.056671  0.016022 -0.035461  0.098572 -0.119873  0.085754  ... -0.073151   \n",
       "4  0.196533 -0.120996 -0.073096 -0.180078 -0.112744  0.060791  ...  0.098340   \n",
       "\n",
       "        292       293       294       295       296       297       298  \\\n",
       "0 -0.065674 -0.117554  0.085938 -0.078674  0.006287 -0.067261 -0.108887   \n",
       "1  0.073608 -0.049805  0.068034 -0.031067 -0.114746 -0.174072  0.117676   \n",
       "2  0.044189 -0.009369 -0.029266  0.012695 -0.024078 -0.188660 -0.022461   \n",
       "3  0.017151  0.017548 -0.055221 -0.057190  0.163560 -0.052979  0.052917   \n",
       "4  0.071582  0.000366  0.184985 -0.114307 -0.017651 -0.054053 -0.059961   \n",
       "\n",
       "        299       300  \n",
       "0  0.037109  0.077209  \n",
       "1  0.026042 -0.007975  \n",
       "2  0.017151  0.098053  \n",
       "3  0.126587 -0.110962  \n",
       "4  0.040894 -0.090448  \n",
       "\n",
       "[5 rows x 301 columns]"
      ]
     },
     "execution_count": 274,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reddit_embed.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "508319"
      ]
     },
     "execution_count": 277,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(reddit_embed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embedding the Ireland headlines\n",
    "ireland_embed = doc_embeddings(ireland_pp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>291</th>\n",
       "      <th>292</th>\n",
       "      <th>293</th>\n",
       "      <th>294</th>\n",
       "      <th>295</th>\n",
       "      <th>296</th>\n",
       "      <th>297</th>\n",
       "      <th>298</th>\n",
       "      <th>299</th>\n",
       "      <th>300</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[egypt, move, close, gaza, border, breach]</td>\n",
       "      <td>-0.038417</td>\n",
       "      <td>0.058207</td>\n",
       "      <td>-0.004354</td>\n",
       "      <td>0.068441</td>\n",
       "      <td>-0.083333</td>\n",
       "      <td>-0.014771</td>\n",
       "      <td>-0.117472</td>\n",
       "      <td>-0.073893</td>\n",
       "      <td>0.112671</td>\n",
       "      <td>...</td>\n",
       "      <td>0.008728</td>\n",
       "      <td>0.131022</td>\n",
       "      <td>-0.095276</td>\n",
       "      <td>-0.015930</td>\n",
       "      <td>-0.051456</td>\n",
       "      <td>0.086187</td>\n",
       "      <td>-0.094309</td>\n",
       "      <td>-0.018880</td>\n",
       "      <td>-0.006632</td>\n",
       "      <td>-0.001099</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[almost, voter, undecided, treaty]</td>\n",
       "      <td>0.285034</td>\n",
       "      <td>0.014404</td>\n",
       "      <td>0.158081</td>\n",
       "      <td>0.135986</td>\n",
       "      <td>-0.093658</td>\n",
       "      <td>-0.060181</td>\n",
       "      <td>0.168396</td>\n",
       "      <td>-0.071533</td>\n",
       "      <td>0.186218</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.027222</td>\n",
       "      <td>-0.081997</td>\n",
       "      <td>0.080460</td>\n",
       "      <td>-0.266479</td>\n",
       "      <td>0.010132</td>\n",
       "      <td>-0.087280</td>\n",
       "      <td>0.035980</td>\n",
       "      <td>0.070435</td>\n",
       "      <td>0.069702</td>\n",
       "      <td>0.003939</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[jacob, fruitfield, factory, lay, staff]</td>\n",
       "      <td>0.031555</td>\n",
       "      <td>-0.072874</td>\n",
       "      <td>0.132446</td>\n",
       "      <td>0.113281</td>\n",
       "      <td>0.062256</td>\n",
       "      <td>0.009979</td>\n",
       "      <td>-0.059769</td>\n",
       "      <td>-0.051270</td>\n",
       "      <td>0.069962</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.145996</td>\n",
       "      <td>0.044922</td>\n",
       "      <td>-0.076172</td>\n",
       "      <td>0.151855</td>\n",
       "      <td>-0.114258</td>\n",
       "      <td>-0.128418</td>\n",
       "      <td>-0.024658</td>\n",
       "      <td>-0.011597</td>\n",
       "      <td>-0.008011</td>\n",
       "      <td>-0.029602</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[bono, say, rich, world, failing, campaign]</td>\n",
       "      <td>0.104574</td>\n",
       "      <td>0.018433</td>\n",
       "      <td>0.097127</td>\n",
       "      <td>0.042318</td>\n",
       "      <td>-0.053141</td>\n",
       "      <td>0.034871</td>\n",
       "      <td>0.052979</td>\n",
       "      <td>-0.130880</td>\n",
       "      <td>0.016996</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.003866</td>\n",
       "      <td>-0.061656</td>\n",
       "      <td>-0.110779</td>\n",
       "      <td>0.008993</td>\n",
       "      <td>-0.029704</td>\n",
       "      <td>-0.027995</td>\n",
       "      <td>0.005948</td>\n",
       "      <td>-0.033732</td>\n",
       "      <td>0.038595</td>\n",
       "      <td>0.034668</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[government, nra, announce, road, plan]</td>\n",
       "      <td>-0.062500</td>\n",
       "      <td>0.051147</td>\n",
       "      <td>0.052444</td>\n",
       "      <td>0.009888</td>\n",
       "      <td>0.028900</td>\n",
       "      <td>-0.152466</td>\n",
       "      <td>-0.021606</td>\n",
       "      <td>-0.046082</td>\n",
       "      <td>0.153847</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.103516</td>\n",
       "      <td>0.014824</td>\n",
       "      <td>-0.034119</td>\n",
       "      <td>-0.089368</td>\n",
       "      <td>-0.026458</td>\n",
       "      <td>-0.048523</td>\n",
       "      <td>-0.080322</td>\n",
       "      <td>0.008057</td>\n",
       "      <td>0.067474</td>\n",
       "      <td>-0.147369</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 301 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           0         1         2         3    \\\n",
       "0   [egypt, move, close, gaza, border, breach] -0.038417  0.058207 -0.004354   \n",
       "1           [almost, voter, undecided, treaty]  0.285034  0.014404  0.158081   \n",
       "2     [jacob, fruitfield, factory, lay, staff]  0.031555 -0.072874  0.132446   \n",
       "3  [bono, say, rich, world, failing, campaign]  0.104574  0.018433  0.097127   \n",
       "4      [government, nra, announce, road, plan] -0.062500  0.051147  0.052444   \n",
       "\n",
       "        4         5         6         7         8         9    ...       291  \\\n",
       "0  0.068441 -0.083333 -0.014771 -0.117472 -0.073893  0.112671  ...  0.008728   \n",
       "1  0.135986 -0.093658 -0.060181  0.168396 -0.071533  0.186218  ... -0.027222   \n",
       "2  0.113281  0.062256  0.009979 -0.059769 -0.051270  0.069962  ... -0.145996   \n",
       "3  0.042318 -0.053141  0.034871  0.052979 -0.130880  0.016996  ... -0.003866   \n",
       "4  0.009888  0.028900 -0.152466 -0.021606 -0.046082  0.153847  ... -0.103516   \n",
       "\n",
       "        292       293       294       295       296       297       298  \\\n",
       "0  0.131022 -0.095276 -0.015930 -0.051456  0.086187 -0.094309 -0.018880   \n",
       "1 -0.081997  0.080460 -0.266479  0.010132 -0.087280  0.035980  0.070435   \n",
       "2  0.044922 -0.076172  0.151855 -0.114258 -0.128418 -0.024658 -0.011597   \n",
       "3 -0.061656 -0.110779  0.008993 -0.029704 -0.027995  0.005948 -0.033732   \n",
       "4  0.014824 -0.034119 -0.089368 -0.026458 -0.048523 -0.080322  0.008057   \n",
       "\n",
       "        299       300  \n",
       "0 -0.006632 -0.001099  \n",
       "1  0.069702  0.003939  \n",
       "2 -0.008011 -0.029602  \n",
       "3  0.038595  0.034668  \n",
       "4  0.067474 -0.147369  \n",
       "\n",
       "[5 rows x 301 columns]"
      ]
     },
     "execution_count": 278,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ireland_embed.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "597058"
      ]
     },
     "execution_count": 279,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ireland_embed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eeR26J-N4-15"
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The headline embeddings are 300 dimensions and so in the process of clustering the headlines we also conduct a dimensionality reduction. We use a UMAP algorithm together with a HDBSCAN clustering method to form hierarchical density based clusters from the word embeddings. \n",
    "\n",
    "The functions defined below were originally written by David Borrelli and can be found at [Clustering sentence embeddings to identify intents in short text](https://towardsdatascience.com/clustering-sentence-embeddings-to-identify-intents-in-short-text-48d22d3bf02e). The function ```random_search``` has been slightly modified to also generate Sihouette scores for each clustering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_clusters(clusters, prob_threshold = 0.05):\n",
    "    \"\"\"\n",
    "    Returns the label count and cost of a given cluster supplied from running hdbscan\n",
    "    \"\"\"\n",
    "    \n",
    "    cluster_labels = clusters.labels_\n",
    "    label_count = len(np.unique(cluster_labels))\n",
    "    total_num = len(clusters.labels_)\n",
    "    cost = (np.count_nonzero(clusters.probabilities_ < prob_threshold)/total_num)\n",
    "    \n",
    "    return label_count, cost\n",
    "\n",
    "\n",
    "def random_search(embeddings, space, num_evals):\n",
    "    \"\"\"\n",
    "    Randomly search hyperparameter space and limited number of times \n",
    "    and return a summary of the results\n",
    "    \"\"\"\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    for i in range(num_evals):\n",
    "\n",
    "        n_neighbors = random.choice(space['n_neighbors'])\n",
    "        n_components = random.choice(space['n_components'])\n",
    "        min_cluster_size = random.choice(space['min_cluster_size'])\n",
    "        \n",
    "        # Dimensionality reduction using UMAP\n",
    "        umap_embedding = umap.UMAP(n_neighbors=n_neighbors, \n",
    "                                n_components=n_components, \n",
    "                                metric='cosine', \n",
    "                                random_state= 42).fit_transform(embeddings.loc[:, 1:].to_numpy())\n",
    "\n",
    "        # Clustering the UMAP reduced word embedding\n",
    "        clusters = hdbscan.HDBSCAN(min_cluster_size = min_cluster_size,\n",
    "                           metric='euclidean', \n",
    "                           cluster_selection_method='eom').fit(umap_embedding)\n",
    "        \n",
    "        # Filtering the noise for the silhouette score\n",
    "        embedding = []\n",
    "        labels = []\n",
    "        for n in range(len(umap_embedding)):\n",
    "            if clusters.labels_[n] != -1:\n",
    "                embedding.append(umap_embedding[n])\n",
    "                labels.append(clusters.labels_[n])\n",
    "        \n",
    "        silhouette_score = metrics.silhouette_score(embedding, labels)\n",
    "        \n",
    "        label_count, cost = score_clusters(clusters, prob_threshold = 0.05)\n",
    "                \n",
    "        results.append([i, n_neighbors, n_components, min_cluster_size, \n",
    "                        label_count, cost, silhouette_score])\n",
    "    \n",
    "    result_df = pd.DataFrame(results, columns=['run_id', 'n_neighbors', 'n_components', \n",
    "                                               'min_cluster_size', 'label_count', 'cost', 'silhouette'])\n",
    "    \n",
    "    return result_df.sort_values(by='cost')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the search space for the random search\n",
    "space = {\"n_neighbors\": range(8,20),\n",
    "        \"n_components\": range(2,7),\n",
    "        \"min_cluster_size\": range(10,30)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reddit News"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>run_id</th>\n",
       "      <th>n_neighbors</th>\n",
       "      <th>n_components</th>\n",
       "      <th>min_cluster_size</th>\n",
       "      <th>label_count</th>\n",
       "      <th>cost</th>\n",
       "      <th>silhouette</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>27</td>\n",
       "      <td>14</td>\n",
       "      <td>3</td>\n",
       "      <td>20</td>\n",
       "      <td>3</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.744467</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>77</td>\n",
       "      <td>15</td>\n",
       "      <td>4</td>\n",
       "      <td>22</td>\n",
       "      <td>3</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.746574</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>25</td>\n",
       "      <td>13</td>\n",
       "      <td>6</td>\n",
       "      <td>15</td>\n",
       "      <td>3</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.752363</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>16</td>\n",
       "      <td>18</td>\n",
       "      <td>4</td>\n",
       "      <td>18</td>\n",
       "      <td>3</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.775728</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>48</td>\n",
       "      <td>13</td>\n",
       "      <td>6</td>\n",
       "      <td>24</td>\n",
       "      <td>3</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.752363</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>29</td>\n",
       "      <td>19</td>\n",
       "      <td>5</td>\n",
       "      <td>14</td>\n",
       "      <td>175</td>\n",
       "      <td>0.585103</td>\n",
       "      <td>0.554993</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>50</td>\n",
       "      <td>14</td>\n",
       "      <td>5</td>\n",
       "      <td>13</td>\n",
       "      <td>215</td>\n",
       "      <td>0.585270</td>\n",
       "      <td>0.559129</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>12</td>\n",
       "      <td>4</td>\n",
       "      <td>19</td>\n",
       "      <td>138</td>\n",
       "      <td>0.585437</td>\n",
       "      <td>0.563620</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>51</td>\n",
       "      <td>12</td>\n",
       "      <td>5</td>\n",
       "      <td>18</td>\n",
       "      <td>150</td>\n",
       "      <td>0.585771</td>\n",
       "      <td>0.571402</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>43</td>\n",
       "      <td>19</td>\n",
       "      <td>6</td>\n",
       "      <td>14</td>\n",
       "      <td>180</td>\n",
       "      <td>0.606470</td>\n",
       "      <td>0.575541</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    run_id  n_neighbors  n_components  min_cluster_size  label_count  \\\n",
       "27      27           14             3                20            3   \n",
       "77      77           15             4                22            3   \n",
       "25      25           13             6                15            3   \n",
       "16      16           18             4                18            3   \n",
       "48      48           13             6                24            3   \n",
       "..     ...          ...           ...               ...          ...   \n",
       "29      29           19             5                14          175   \n",
       "50      50           14             5                13          215   \n",
       "2        2           12             4                19          138   \n",
       "51      51           12             5                18          150   \n",
       "43      43           19             6                14          180   \n",
       "\n",
       "        cost  silhouette  \n",
       "27  0.000000    0.744467  \n",
       "77  0.000000    0.746574  \n",
       "25  0.000000    0.752363  \n",
       "16  0.000000    0.775728  \n",
       "48  0.000000    0.752363  \n",
       "..       ...         ...  \n",
       "29  0.585103    0.554993  \n",
       "50  0.585270    0.559129  \n",
       "2   0.585437    0.563620  \n",
       "51  0.585771    0.571402  \n",
       "43  0.606470    0.575541  \n",
       "\n",
       "[100 rows x 7 columns]"
      ]
     },
     "execution_count": 262,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Running the random parameter search on the Reddit news\n",
    "reddit_param_search = random_search(embeddings=reddit_embed, space=space, num_evals= 100)\n",
    "reddit_param_search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "run_id               35.000000\n",
       "n_neighbors           8.000000\n",
       "n_components          3.000000\n",
       "min_cluster_size     20.000000\n",
       "label_count         155.000000\n",
       "cost                  0.428971\n",
       "silhouette            0.534276\n",
       "Name: 0, dtype: float64"
      ]
     },
     "execution_count": 295,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cluster that was decided upon. Note that label count_uncludes \"unlabeled\" data as a label. \n",
    "# We remove this label from the label count in our presentation of results in the report.\n",
    "reddit_param_search.iloc[65]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate the optimized cluster model for the Reddit data\n",
    "reddit_umap_embedding = umap.UMAP(n_neighbors=8, \n",
    "                                n_components=3, \n",
    "                                metric='cosine', \n",
    "                                # Changed original function to fit the format of our data.\n",
    "                                random_state=42).fit_transform(reddit_embed.loc[:, 1:].to_numpy())\n",
    "\n",
    "reddit_clusters = hdbscan.HDBSCAN(min_cluster_size = 20,\n",
    "                           metric='euclidean', \n",
    "                           cluster_selection_method='eom').fit(reddit_umap_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/norahahr/opt/anaconda3/lib/python3.8/site-packages/umap/spectral.py:260: UserWarning: WARNING: spectral initialisation failed! The eigenvector solver\n",
      "failed. This is likely due to too small an eigengap. Consider\n",
      "adding some noise or jitter to your data.\n",
      "\n",
      "Falling back to random initialisation!\n",
      "  warn(\n",
      "/Users/norahahr/opt/anaconda3/lib/python3.8/site-packages/umap/spectral.py:260: UserWarning: WARNING: spectral initialisation failed! The eigenvector solver\n",
      "failed. This is likely due to too small an eigengap. Consider\n",
      "adding some noise or jitter to your data.\n",
      "\n",
      "Falling back to random initialisation!\n",
      "  warn(\n",
      "/Users/norahahr/opt/anaconda3/lib/python3.8/site-packages/umap/spectral.py:260: UserWarning: WARNING: spectral initialisation failed! The eigenvector solver\n",
      "failed. This is likely due to too small an eigengap. Consider\n",
      "adding some noise or jitter to your data.\n",
      "\n",
      "Falling back to random initialisation!\n",
      "  warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>run_id</th>\n",
       "      <th>n_neighbors</th>\n",
       "      <th>n_components</th>\n",
       "      <th>min_cluster_size</th>\n",
       "      <th>label_count</th>\n",
       "      <th>cost</th>\n",
       "      <th>silhouette</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>35</td>\n",
       "      <td>13</td>\n",
       "      <td>2</td>\n",
       "      <td>28</td>\n",
       "      <td>11</td>\n",
       "      <td>0.013849</td>\n",
       "      <td>0.312246</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>16</td>\n",
       "      <td>18</td>\n",
       "      <td>6</td>\n",
       "      <td>26</td>\n",
       "      <td>10</td>\n",
       "      <td>0.015511</td>\n",
       "      <td>0.401176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>41</td>\n",
       "      <td>19</td>\n",
       "      <td>6</td>\n",
       "      <td>29</td>\n",
       "      <td>11</td>\n",
       "      <td>0.015712</td>\n",
       "      <td>0.351361</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>90</td>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "      <td>13</td>\n",
       "      <td>283</td>\n",
       "      <td>0.387672</td>\n",
       "      <td>0.594165</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>11</td>\n",
       "      <td>12</td>\n",
       "      <td>2</td>\n",
       "      <td>10</td>\n",
       "      <td>317</td>\n",
       "      <td>0.392456</td>\n",
       "      <td>0.562624</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>73</td>\n",
       "      <td>12</td>\n",
       "      <td>5</td>\n",
       "      <td>27</td>\n",
       "      <td>95</td>\n",
       "      <td>0.567759</td>\n",
       "      <td>0.618530</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>29</td>\n",
       "      <td>19</td>\n",
       "      <td>6</td>\n",
       "      <td>23</td>\n",
       "      <td>109</td>\n",
       "      <td>0.570277</td>\n",
       "      <td>0.624632</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>18</td>\n",
       "      <td>19</td>\n",
       "      <td>3</td>\n",
       "      <td>25</td>\n",
       "      <td>101</td>\n",
       "      <td>0.575263</td>\n",
       "      <td>0.596092</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>93</td>\n",
       "      <td>15</td>\n",
       "      <td>6</td>\n",
       "      <td>29</td>\n",
       "      <td>82</td>\n",
       "      <td>0.580299</td>\n",
       "      <td>0.588972</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>14</td>\n",
       "      <td>5</td>\n",
       "      <td>28</td>\n",
       "      <td>88</td>\n",
       "      <td>0.582968</td>\n",
       "      <td>0.624097</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    run_id  n_neighbors  n_components  min_cluster_size  label_count  \\\n",
       "35      35           13             2                28           11   \n",
       "16      16           18             6                26           10   \n",
       "41      41           19             6                29           11   \n",
       "90      90            8             2                13          283   \n",
       "11      11           12             2                10          317   \n",
       "..     ...          ...           ...               ...          ...   \n",
       "73      73           12             5                27           95   \n",
       "29      29           19             6                23          109   \n",
       "18      18           19             3                25          101   \n",
       "93      93           15             6                29           82   \n",
       "1        1           14             5                28           88   \n",
       "\n",
       "        cost  silhouette  \n",
       "35  0.013849    0.312246  \n",
       "16  0.015511    0.401176  \n",
       "41  0.015712    0.351361  \n",
       "90  0.387672    0.594165  \n",
       "11  0.392456    0.562624  \n",
       "..       ...         ...  \n",
       "73  0.567759    0.618530  \n",
       "29  0.570277    0.624632  \n",
       "18  0.575263    0.596092  \n",
       "93  0.580299    0.588972  \n",
       "1   0.582968    0.624097  \n",
       "\n",
       "[100 rows x 7 columns]"
      ]
     },
     "execution_count": 280,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Running the random parameter search on the Ireland news\n",
    "ireland_param_search = random_search(embeddings=ireland_embed, space=space, num_evals= 100)\n",
    "ireland_param_search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "run_id               68.000000\n",
       "n_neighbors          12.000000\n",
       "n_components          2.000000\n",
       "min_cluster_size     23.000000\n",
       "label_count         132.000000\n",
       "cost                  0.460341\n",
       "silhouette            0.558982\n",
       "Name: 68, dtype: float64"
      ]
     },
     "execution_count": 299,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cluster that was decided upon. Note that label_count uncludes \"unlabeled\" data as a label. \n",
    "# We remove this label from the label count in our presentation of results in the report.\n",
    "ireland_param_search.iloc[17]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [],
   "source": [
    "ireland_umap_embedding = umap.UMAP(n_neighbors=12, \n",
    "                                n_components=2, \n",
    "                                metric='cosine', \n",
    "                                # Changed original function to fit the format of our data.\n",
    "                                random_state=42).fit_transform(ireland_embed.loc[:, 1:].to_numpy())\n",
    "\n",
    "ireland_clusters = hdbscan.HDBSCAN(min_cluster_size = 23,\n",
    "                           metric='euclidean', \n",
    "                           cluster_selection_method='eom').fit(ireland_umap_embedding)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cluster Labeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When labeling the clusters we looked at the 20 most common words in each cluster as well as 15 randomly sampled headlines from the cluster. Based on these words we derived the theme or topic of that cluster as outlined in the report. Below, are the functions used in the labeling process and a summary of the 20 most common words for each cluster from each dataset. We have also provided an example of 15 sample sentences from one cluster from each dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cluster_words(embedding, clustering, label, top_words = 20):\n",
    "    \"\"\"Given a list of sentence embeddings and correspoding cluster labels, returns the most frequent words\n",
    "   for a given cluster.\"\"\"\n",
    "    \n",
    "    word_list = []\n",
    "\n",
    "    for n in range(len(embedding)):\n",
    "        if clustering.labels_[n] == label:\n",
    "            word_list.append(embedding.iloc[n][0])\n",
    "\n",
    "    word_counter = Counter(chain.from_iterable(word_list))\n",
    "\n",
    "    return word_counter.most_common(top_words)\n",
    "\n",
    "def clusters_words(embedding, clustering, top_words = 20):\n",
    "    \"\"\"Given a list of sentence embeddings and corresponding cluster labels, returns the most frequent words\n",
    "    for each cluster.\"\"\"\n",
    "    \n",
    "    word_freq_df = pd.DataFrame()\n",
    "    \n",
    "    for n in np.unique(clustering.labels_):\n",
    "        words_df = pd.Series(cluster_words(embedding = embedding, \n",
    "                                          clustering = clustering, \n",
    "                                          label = n, \n",
    "                                          top_words = top_words))\n",
    "        cluster_df = pd.Series(n).append(words_df, ignore_index = True)\n",
    "        word_freq_df = word_freq_df.append(cluster_df, ignore_index=True)\n",
    "        \n",
    "    return word_freq_df\n",
    "\n",
    "def get_headlines(clusters, label, embedding, number = 0):\n",
    "    \"\"\"Given a clustering, a cluster label, and the corresponding headline embeddings,\n",
    "    return a given number of random preprocessed headlines from the cluster. If the number of headlines\n",
    "    is left unspecified, return all headlines in the cluster\"\"\"\n",
    "    \n",
    "    if number == 0:\n",
    "        number = len(cluster_size(clusters = clusters, label = label))\n",
    "        \n",
    "    headlines = []\n",
    "    \n",
    "    for n in range(len(embedding)):\n",
    "        if clusters.labels_[n] == label:\n",
    "            headlines.append(embedding.iloc[n][0])\n",
    "    \n",
    "    return random.sample(headlines, number)\n",
    "\n",
    "def cluster_size (clusters, label):\n",
    "    \"\"\"Given a clustering and a cluster label, return the size of that cluster\"\"\"\n",
    "    return Counter(clusters.labels_)[label]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The top 20 most common words for each cluster in the Reddit dataset\n",
    "reddit_freq = clusters_words(embedding= reddit_embed, clustering=reddit_clusters, top_words = 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>(israel, 816)</td>\n",
       "      <td>(gaza, 724)</td>\n",
       "      <td>(israeli, 582)</td>\n",
       "      <td>(say, 567)</td>\n",
       "      <td>(world, 566)</td>\n",
       "      <td>(war, 539)</td>\n",
       "      <td>(year, 494)</td>\n",
       "      <td>(new, 490)</td>\n",
       "      <td>(iraq, 472)</td>\n",
       "      <td>...</td>\n",
       "      <td>(attack, 414)</td>\n",
       "      <td>(government, 403)</td>\n",
       "      <td>(people, 393)</td>\n",
       "      <td>(police, 370)</td>\n",
       "      <td>(video, 368)</td>\n",
       "      <td>(child, 361)</td>\n",
       "      <td>(pakistan, 354)</td>\n",
       "      <td>(one, 338)</td>\n",
       "      <td>(woman, 327)</td>\n",
       "      <td>(iran, 323)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>(evening, 42)</td>\n",
       "      <td>(summary, 42)</td>\n",
       "      <td>(news, 21)</td>\n",
       "      <td>(flash, 21)</td>\n",
       "      <td>(quick, 21)</td>\n",
       "      <td>(story, 21)</td>\n",
       "      <td>(link, 21)</td>\n",
       "      <td>(associated, 21)</td>\n",
       "      <td>(article, 21)</td>\n",
       "      <td>...</td>\n",
       "      <td>(quote, 4)</td>\n",
       "      <td>(day, 4)</td>\n",
       "      <td>(february, 3)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>(summary, 63)</td>\n",
       "      <td>(evening, 62)</td>\n",
       "      <td>(story, 32)</td>\n",
       "      <td>(news, 31)</td>\n",
       "      <td>(flash, 31)</td>\n",
       "      <td>(quick, 31)</td>\n",
       "      <td>(link, 31)</td>\n",
       "      <td>(associated, 31)</td>\n",
       "      <td>(article, 31)</td>\n",
       "      <td>...</td>\n",
       "      <td>(october, 5)</td>\n",
       "      <td>(target, 1)</td>\n",
       "      <td>(blogger, 1)</td>\n",
       "      <td>(excerpt, 1)</td>\n",
       "      <td>(backgrounder, 1)</td>\n",
       "      <td>(basic, 1)</td>\n",
       "      <td>(fact, 1)</td>\n",
       "      <td>(abkhazia, 1)</td>\n",
       "      <td>(chronicle, 1)</td>\n",
       "      <td>(injustice, 1)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2.0</td>\n",
       "      <td>(jimmy, 17)</td>\n",
       "      <td>(carter, 16)</td>\n",
       "      <td>(hadron, 5)</td>\n",
       "      <td>(collider, 5)</td>\n",
       "      <td>(hamas, 4)</td>\n",
       "      <td>(large, 4)</td>\n",
       "      <td>(meet, 2)</td>\n",
       "      <td>(terrorist, 2)</td>\n",
       "      <td>(leader, 2)</td>\n",
       "      <td>...</td>\n",
       "      <td>(barred, 2)</td>\n",
       "      <td>(zimbabwe, 2)</td>\n",
       "      <td>(shame, 1)</td>\n",
       "      <td>(news, 1)</td>\n",
       "      <td>(madlibs, 1)</td>\n",
       "      <td>(defends, 1)</td>\n",
       "      <td>(meeting, 1)</td>\n",
       "      <td>(going, 1)</td>\n",
       "      <td>(israeli, 1)</td>\n",
       "      <td>(snub, 1)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3.0</td>\n",
       "      <td>(bin, 82)</td>\n",
       "      <td>(laden, 78)</td>\n",
       "      <td>(osama, 24)</td>\n",
       "      <td>(urge, 8)</td>\n",
       "      <td>(son, 7)</td>\n",
       "      <td>(jihad, 7)</td>\n",
       "      <td>(dead, 6)</td>\n",
       "      <td>(tape, 6)</td>\n",
       "      <td>(israel, 6)</td>\n",
       "      <td>...</td>\n",
       "      <td>(driver, 5)</td>\n",
       "      <td>(guantanamo, 4)</td>\n",
       "      <td>(still, 4)</td>\n",
       "      <td>(bush, 4)</td>\n",
       "      <td>(call, 4)</td>\n",
       "      <td>(yemen, 4)</td>\n",
       "      <td>(obama, 4)</td>\n",
       "      <td>(trial, 4)</td>\n",
       "      <td>(breaking, 3)</td>\n",
       "      <td>(video, 3)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>150</th>\n",
       "      <td>149.0</td>\n",
       "      <td>(prison, 64)</td>\n",
       "      <td>(year, 37)</td>\n",
       "      <td>(jail, 28)</td>\n",
       "      <td>(sentenced, 26)</td>\n",
       "      <td>(sentence, 25)</td>\n",
       "      <td>(court, 20)</td>\n",
       "      <td>(jailed, 19)</td>\n",
       "      <td>(murder, 19)</td>\n",
       "      <td>(death, 14)</td>\n",
       "      <td>...</td>\n",
       "      <td>(journalist, 13)</td>\n",
       "      <td>(guilty, 12)</td>\n",
       "      <td>(jury, 12)</td>\n",
       "      <td>(four, 9)</td>\n",
       "      <td>(convicted, 9)</td>\n",
       "      <td>(man, 9)</td>\n",
       "      <td>(face, 9)</td>\n",
       "      <td>(released, 8)</td>\n",
       "      <td>(judge, 8)</td>\n",
       "      <td>(two, 8)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>151</th>\n",
       "      <td>150.0</td>\n",
       "      <td>(detainee, 35)</td>\n",
       "      <td>(guantanamo, 14)</td>\n",
       "      <td>(court, 10)</td>\n",
       "      <td>(gitmo, 9)</td>\n",
       "      <td>(enemy, 5)</td>\n",
       "      <td>(combatant, 5)</td>\n",
       "      <td>(say, 5)</td>\n",
       "      <td>(prisoner, 5)</td>\n",
       "      <td>(judge, 4)</td>\n",
       "      <td>...</td>\n",
       "      <td>(release, 4)</td>\n",
       "      <td>(abu, 4)</td>\n",
       "      <td>(tortured, 4)</td>\n",
       "      <td>(case, 4)</td>\n",
       "      <td>(supreme, 3)</td>\n",
       "      <td>(ghraib, 3)</td>\n",
       "      <td>(former, 3)</td>\n",
       "      <td>(inmate, 3)</td>\n",
       "      <td>(sue, 3)</td>\n",
       "      <td>(denied, 3)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>152</th>\n",
       "      <td>151.0</td>\n",
       "      <td>(war, 20)</td>\n",
       "      <td>(crime, 20)</td>\n",
       "      <td>(court, 16)</td>\n",
       "      <td>(probe, 11)</td>\n",
       "      <td>(warrant, 11)</td>\n",
       "      <td>(prosecutor, 10)</td>\n",
       "      <td>(president, 10)</td>\n",
       "      <td>(international, 10)</td>\n",
       "      <td>(criminal, 9)</td>\n",
       "      <td>...</td>\n",
       "      <td>(sudan, 7)</td>\n",
       "      <td>(former, 6)</td>\n",
       "      <td>(tribunal, 6)</td>\n",
       "      <td>(fraud, 5)</td>\n",
       "      <td>(call, 5)</td>\n",
       "      <td>(israeli, 5)</td>\n",
       "      <td>(darfur, 5)</td>\n",
       "      <td>(karadzic, 5)</td>\n",
       "      <td>(face, 4)</td>\n",
       "      <td>(leader, 4)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153</th>\n",
       "      <td>152.0</td>\n",
       "      <td>(trial, 14)</td>\n",
       "      <td>(detention, 7)</td>\n",
       "      <td>(guantanamo, 5)</td>\n",
       "      <td>(american, 5)</td>\n",
       "      <td>(former, 4)</td>\n",
       "      <td>(prosecutor, 3)</td>\n",
       "      <td>(genocide, 3)</td>\n",
       "      <td>(start, 3)</td>\n",
       "      <td>(terror, 2)</td>\n",
       "      <td>...</td>\n",
       "      <td>(world, 2)</td>\n",
       "      <td>(limit, 2)</td>\n",
       "      <td>(day, 2)</td>\n",
       "      <td>(jail, 2)</td>\n",
       "      <td>(terrorism, 2)</td>\n",
       "      <td>(suspect, 2)</td>\n",
       "      <td>(torture, 2)</td>\n",
       "      <td>(charge, 2)</td>\n",
       "      <td>(terrorist, 2)</td>\n",
       "      <td>(citizen, 2)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>154</th>\n",
       "      <td>153.0</td>\n",
       "      <td>(torture, 40)</td>\n",
       "      <td>(waterboarding, 10)</td>\n",
       "      <td>(cia, 8)</td>\n",
       "      <td>(bush, 6)</td>\n",
       "      <td>(video, 4)</td>\n",
       "      <td>(release, 4)</td>\n",
       "      <td>(new, 3)</td>\n",
       "      <td>(gitmo, 3)</td>\n",
       "      <td>(white, 3)</td>\n",
       "      <td>...</td>\n",
       "      <td>(ban, 3)</td>\n",
       "      <td>(american, 3)</td>\n",
       "      <td>(held, 3)</td>\n",
       "      <td>(abuse, 3)</td>\n",
       "      <td>(guantanamo, 3)</td>\n",
       "      <td>(evidence, 3)</td>\n",
       "      <td>(charge, 2)</td>\n",
       "      <td>(time, 2)</td>\n",
       "      <td>(mccain, 2)</td>\n",
       "      <td>(standing, 2)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>155 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        0               1                    2                3   \\\n",
       "0     -1.0   (israel, 816)          (gaza, 724)   (israeli, 582)   \n",
       "1      0.0   (evening, 42)        (summary, 42)       (news, 21)   \n",
       "2      1.0   (summary, 63)        (evening, 62)      (story, 32)   \n",
       "3      2.0     (jimmy, 17)         (carter, 16)      (hadron, 5)   \n",
       "4      3.0       (bin, 82)          (laden, 78)      (osama, 24)   \n",
       "..     ...             ...                  ...              ...   \n",
       "150  149.0    (prison, 64)           (year, 37)       (jail, 28)   \n",
       "151  150.0  (detainee, 35)     (guantanamo, 14)      (court, 10)   \n",
       "152  151.0       (war, 20)          (crime, 20)      (court, 16)   \n",
       "153  152.0     (trial, 14)       (detention, 7)  (guantanamo, 5)   \n",
       "154  153.0   (torture, 40)  (waterboarding, 10)         (cia, 8)   \n",
       "\n",
       "                  4               5                 6                7   \\\n",
       "0         (say, 567)    (world, 566)        (war, 539)      (year, 494)   \n",
       "1        (flash, 21)     (quick, 21)       (story, 21)       (link, 21)   \n",
       "2         (news, 31)     (flash, 31)       (quick, 31)       (link, 31)   \n",
       "3      (collider, 5)      (hamas, 4)        (large, 4)        (meet, 2)   \n",
       "4          (urge, 8)        (son, 7)        (jihad, 7)        (dead, 6)   \n",
       "..               ...             ...               ...              ...   \n",
       "150  (sentenced, 26)  (sentence, 25)       (court, 20)     (jailed, 19)   \n",
       "151       (gitmo, 9)      (enemy, 5)    (combatant, 5)         (say, 5)   \n",
       "152      (probe, 11)   (warrant, 11)  (prosecutor, 10)  (president, 10)   \n",
       "153    (american, 5)     (former, 4)   (prosecutor, 3)    (genocide, 3)   \n",
       "154        (bush, 6)      (video, 4)      (release, 4)         (new, 3)   \n",
       "\n",
       "                      8              9   ...                11  \\\n",
       "0             (new, 490)    (iraq, 472)  ...     (attack, 414)   \n",
       "1       (associated, 21)  (article, 21)  ...        (quote, 4)   \n",
       "2       (associated, 31)  (article, 31)  ...      (october, 5)   \n",
       "3         (terrorist, 2)    (leader, 2)  ...       (barred, 2)   \n",
       "4              (tape, 6)    (israel, 6)  ...       (driver, 5)   \n",
       "..                   ...            ...  ...               ...   \n",
       "150         (murder, 19)    (death, 14)  ...  (journalist, 13)   \n",
       "151        (prisoner, 5)     (judge, 4)  ...      (release, 4)   \n",
       "152  (international, 10)  (criminal, 9)  ...        (sudan, 7)   \n",
       "153           (start, 3)    (terror, 2)  ...        (world, 2)   \n",
       "154           (gitmo, 3)     (white, 3)  ...          (ban, 3)   \n",
       "\n",
       "                    12             13             14                 15  \\\n",
       "0    (government, 403)  (people, 393)  (police, 370)       (video, 368)   \n",
       "1             (day, 4)  (february, 3)            NaN                NaN   \n",
       "2          (target, 1)   (blogger, 1)   (excerpt, 1)  (backgrounder, 1)   \n",
       "3        (zimbabwe, 2)     (shame, 1)      (news, 1)       (madlibs, 1)   \n",
       "4      (guantanamo, 4)     (still, 4)      (bush, 4)          (call, 4)   \n",
       "..                 ...            ...            ...                ...   \n",
       "150       (guilty, 12)     (jury, 12)      (four, 9)     (convicted, 9)   \n",
       "151           (abu, 4)  (tortured, 4)      (case, 4)       (supreme, 3)   \n",
       "152        (former, 6)  (tribunal, 6)     (fraud, 5)          (call, 5)   \n",
       "153         (limit, 2)       (day, 2)      (jail, 2)     (terrorism, 2)   \n",
       "154      (american, 3)      (held, 3)     (abuse, 3)    (guantanamo, 3)   \n",
       "\n",
       "                16               17             18              19  \\\n",
       "0     (child, 361)  (pakistan, 354)     (one, 338)    (woman, 327)   \n",
       "1              NaN              NaN            NaN             NaN   \n",
       "2       (basic, 1)        (fact, 1)  (abkhazia, 1)  (chronicle, 1)   \n",
       "3     (defends, 1)     (meeting, 1)     (going, 1)    (israeli, 1)   \n",
       "4       (yemen, 4)       (obama, 4)     (trial, 4)   (breaking, 3)   \n",
       "..             ...              ...            ...             ...   \n",
       "150       (man, 9)        (face, 9)  (released, 8)      (judge, 8)   \n",
       "151    (ghraib, 3)      (former, 3)    (inmate, 3)        (sue, 3)   \n",
       "152   (israeli, 5)      (darfur, 5)  (karadzic, 5)       (face, 4)   \n",
       "153   (suspect, 2)     (torture, 2)    (charge, 2)  (terrorist, 2)   \n",
       "154  (evidence, 3)      (charge, 2)      (time, 2)     (mccain, 2)   \n",
       "\n",
       "                 20  \n",
       "0       (iran, 323)  \n",
       "1               NaN  \n",
       "2    (injustice, 1)  \n",
       "3         (snub, 1)  \n",
       "4        (video, 3)  \n",
       "..              ...  \n",
       "150        (two, 8)  \n",
       "151     (denied, 3)  \n",
       "152     (leader, 4)  \n",
       "153    (citizen, 2)  \n",
       "154   (standing, 2)  \n",
       "\n",
       "[155 rows x 21 columns]"
      ]
     },
     "execution_count": 310,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reddit_freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['sept',\n",
       "  'victim',\n",
       "  'seek',\n",
       "  'tie',\n",
       "  'osama',\n",
       "  'bin',\n",
       "  'laden',\n",
       "  'family',\n",
       "  'business',\n",
       "  'share',\n",
       "  'vast',\n",
       "  'fortune'],\n",
       " ['islamic',\n",
       "  'jihadist',\n",
       "  'group',\n",
       "  'gspc',\n",
       "  'claim',\n",
       "  'potent',\n",
       "  'osama',\n",
       "  'bin',\n",
       "  'laden',\n",
       "  'affiliate'],\n",
       " ['target', 'bin', 'laden'],\n",
       " ['come', 'gaza', 'jihad', 'spearheaded', 'old', 'friend', 'bin', 'laden'],\n",
       " ['bin', 'laden', 'slam', 'prophet', 'cartoon'],\n",
       " ['obama', 'bin', 'laden', 'still', 'free', 'gop'],\n",
       " ['bin', 'laden', 'videomaker', 'face', 'life', 'gitmo', 'trial'],\n",
       " ['osama', 'bin', 'laden', 'writing', 'memoir', 'report'],\n",
       " ['love', 'love', 'sheikh', 'osama', 'bin', 'laden', 'kill', 'america'],\n",
       " ['white', 'house', 'blocking', 'search', 'bin', 'laden'],\n",
       " ['bin', 'laden', 'happy', 'september', 'toll', 'war', 'court', 'told'],\n",
       " ['cia',\n",
       "  'transfer',\n",
       "  'suspected',\n",
       "  'qaeda',\n",
       "  'member',\n",
       "  'close',\n",
       "  'tie',\n",
       "  'osama',\n",
       "  'bin',\n",
       "  'laden',\n",
       "  'guantanamo'],\n",
       " ['omar', 'bin', 'laden', 'made', 'copy', 'father'],\n",
       " ['osama', 'bin', 'laden', 'found'],\n",
       " ['bush', 'go', 'release', 'bin', 'laden', 'man', 'coincidence']]"
      ]
     },
     "execution_count": 316,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_headlines(clusters=reddit_clusters, label=3, embedding=reddit_embed, number = 15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The top 20 most common words in each cluster in the Ireland dataset\n",
    "ireland_freq = clusters_words(embedding= ireland_embed, clustering=ireland_clusters, top_words = 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>(new, 262)</td>\n",
       "      <td>(say, 233)</td>\n",
       "      <td>(irish, 190)</td>\n",
       "      <td>(man, 188)</td>\n",
       "      <td>(take, 146)</td>\n",
       "      <td>(call, 143)</td>\n",
       "      <td>(ahern, 134)</td>\n",
       "      <td>(plan, 132)</td>\n",
       "      <td>(get, 131)</td>\n",
       "      <td>...</td>\n",
       "      <td>(set, 129)</td>\n",
       "      <td>(back, 126)</td>\n",
       "      <td>(court, 120)</td>\n",
       "      <td>(home, 114)</td>\n",
       "      <td>(talk, 108)</td>\n",
       "      <td>(report, 106)</td>\n",
       "      <td>(case, 105)</td>\n",
       "      <td>(year, 105)</td>\n",
       "      <td>(make, 104)</td>\n",
       "      <td>(time, 99)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>(diary, 93)</td>\n",
       "      <td>(irishman, 79)</td>\n",
       "      <td>(irishwoman, 7)</td>\n",
       "      <td>(dead, 1)</td>\n",
       "      <td>(arminta, 1)</td>\n",
       "      <td>(wallace, 1)</td>\n",
       "      <td>(walk, 1)</td>\n",
       "      <td>(may, 1)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>(short, 220)</td>\n",
       "      <td>(mossbank, 1)</td>\n",
       "      <td>(come, 1)</td>\n",
       "      <td>(long, 1)</td>\n",
       "      <td>(lane, 1)</td>\n",
       "      <td>(hop, 1)</td>\n",
       "      <td>(sharp, 1)</td>\n",
       "      <td>(chest, 1)</td>\n",
       "      <td>(shock, 1)</td>\n",
       "      <td>...</td>\n",
       "      <td>(sight, 1)</td>\n",
       "      <td>(drive, 1)</td>\n",
       "      <td>(ulster, 1)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2.0</td>\n",
       "      <td>(paperback, 12)</td>\n",
       "      <td>(noticeboard, 11)</td>\n",
       "      <td>(leaflet, 2)</td>\n",
       "      <td>(ban, 1)</td>\n",
       "      <td>(advert, 1)</td>\n",
       "      <td>(dublin, 1)</td>\n",
       "      <td>(city, 1)</td>\n",
       "      <td>(centre, 1)</td>\n",
       "      <td>(concern, 1)</td>\n",
       "      <td>...</td>\n",
       "      <td>(lisbon, 1)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3.0</td>\n",
       "      <td>(movie, 15)</td>\n",
       "      <td>(weekly, 14)</td>\n",
       "      <td>(quiz, 14)</td>\n",
       "      <td>(lifeline, 14)</td>\n",
       "      <td>(telly, 1)</td>\n",
       "      <td>(version, 1)</td>\n",
       "      <td>(better, 1)</td>\n",
       "      <td>(throwing, 1)</td>\n",
       "      <td>(waterford, 1)</td>\n",
       "      <td>...</td>\n",
       "      <td>(make, 1)</td>\n",
       "      <td>(sense, 1)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>127</th>\n",
       "      <td>126.0</td>\n",
       "      <td>(green, 18)</td>\n",
       "      <td>(revolution, 5)</td>\n",
       "      <td>(light, 4)</td>\n",
       "      <td>(rub, 3)</td>\n",
       "      <td>(recovery, 3)</td>\n",
       "      <td>(ahern, 2)</td>\n",
       "      <td>(party, 2)</td>\n",
       "      <td>(shade, 2)</td>\n",
       "      <td>(leader, 1)</td>\n",
       "      <td>...</td>\n",
       "      <td>(hoping, 1)</td>\n",
       "      <td>(spec, 1)</td>\n",
       "      <td>(promise, 1)</td>\n",
       "      <td>(new, 1)</td>\n",
       "      <td>(insight, 1)</td>\n",
       "      <td>(lady, 1)</td>\n",
       "      <td>(red, 1)</td>\n",
       "      <td>(shoot, 1)</td>\n",
       "      <td>(reappear, 1)</td>\n",
       "      <td>(tipperary, 1)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128</th>\n",
       "      <td>127.0</td>\n",
       "      <td>(red, 18)</td>\n",
       "      <td>(hot, 11)</td>\n",
       "      <td>(blue, 9)</td>\n",
       "      <td>(light, 9)</td>\n",
       "      <td>(black, 5)</td>\n",
       "      <td>(card, 5)</td>\n",
       "      <td>(make, 5)</td>\n",
       "      <td>(cold, 3)</td>\n",
       "      <td>(chip, 3)</td>\n",
       "      <td>...</td>\n",
       "      <td>(get, 3)</td>\n",
       "      <td>(diamond, 3)</td>\n",
       "      <td>(still, 3)</td>\n",
       "      <td>(dublin, 3)</td>\n",
       "      <td>(see, 3)</td>\n",
       "      <td>(success, 3)</td>\n",
       "      <td>(side, 3)</td>\n",
       "      <td>(brown, 3)</td>\n",
       "      <td>(time, 2)</td>\n",
       "      <td>(ability, 2)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>129</th>\n",
       "      <td>128.0</td>\n",
       "      <td>(keep, 18)</td>\n",
       "      <td>(record, 14)</td>\n",
       "      <td>(point, 12)</td>\n",
       "      <td>(arsenal, 10)</td>\n",
       "      <td>(clear, 8)</td>\n",
       "      <td>(alive, 8)</td>\n",
       "      <td>(move, 7)</td>\n",
       "      <td>(winning, 7)</td>\n",
       "      <td>(strong, 6)</td>\n",
       "      <td>...</td>\n",
       "      <td>(hope, 5)</td>\n",
       "      <td>(canning, 5)</td>\n",
       "      <td>(track, 5)</td>\n",
       "      <td>(prove, 5)</td>\n",
       "      <td>(luck, 4)</td>\n",
       "      <td>(pole, 4)</td>\n",
       "      <td>(position, 4)</td>\n",
       "      <td>(brace, 4)</td>\n",
       "      <td>(formula, 4)</td>\n",
       "      <td>(put, 4)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>130</th>\n",
       "      <td>129.0</td>\n",
       "      <td>(win, 7)</td>\n",
       "      <td>(united, 6)</td>\n",
       "      <td>(patrick, 6)</td>\n",
       "      <td>(back, 4)</td>\n",
       "      <td>(defeat, 4)</td>\n",
       "      <td>(fight, 4)</td>\n",
       "      <td>(keep, 3)</td>\n",
       "      <td>(title, 3)</td>\n",
       "      <td>(singh, 2)</td>\n",
       "      <td>...</td>\n",
       "      <td>(front, 2)</td>\n",
       "      <td>(alive, 2)</td>\n",
       "      <td>(game, 2)</td>\n",
       "      <td>(say, 2)</td>\n",
       "      <td>(run, 2)</td>\n",
       "      <td>(walsh, 2)</td>\n",
       "      <td>(award, 2)</td>\n",
       "      <td>(sullivan, 2)</td>\n",
       "      <td>(clear, 2)</td>\n",
       "      <td>(russell, 2)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131</th>\n",
       "      <td>130.0</td>\n",
       "      <td>(win, 50)</td>\n",
       "      <td>(title, 32)</td>\n",
       "      <td>(lead, 26)</td>\n",
       "      <td>(victory, 21)</td>\n",
       "      <td>(seal, 15)</td>\n",
       "      <td>(claim, 14)</td>\n",
       "      <td>(champion, 11)</td>\n",
       "      <td>(derry, 10)</td>\n",
       "      <td>(late, 9)</td>\n",
       "      <td>...</td>\n",
       "      <td>(finish, 7)</td>\n",
       "      <td>(first, 6)</td>\n",
       "      <td>(top, 6)</td>\n",
       "      <td>(take, 5)</td>\n",
       "      <td>(point, 5)</td>\n",
       "      <td>(comeback, 5)</td>\n",
       "      <td>(snatch, 5)</td>\n",
       "      <td>(world, 5)</td>\n",
       "      <td>(pat, 5)</td>\n",
       "      <td>(two, 4)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>132 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        0                1                  2                3   \\\n",
       "0     -1.0       (new, 262)         (say, 233)     (irish, 190)   \n",
       "1      0.0      (diary, 93)     (irishman, 79)  (irishwoman, 7)   \n",
       "2      1.0     (short, 220)      (mossbank, 1)        (come, 1)   \n",
       "3      2.0  (paperback, 12)  (noticeboard, 11)     (leaflet, 2)   \n",
       "4      3.0      (movie, 15)       (weekly, 14)       (quiz, 14)   \n",
       "..     ...              ...                ...              ...   \n",
       "127  126.0      (green, 18)    (revolution, 5)       (light, 4)   \n",
       "128  127.0        (red, 18)          (hot, 11)        (blue, 9)   \n",
       "129  128.0       (keep, 18)       (record, 14)      (point, 12)   \n",
       "130  129.0         (win, 7)        (united, 6)     (patrick, 6)   \n",
       "131  130.0        (win, 50)        (title, 32)       (lead, 26)   \n",
       "\n",
       "                 4              5             6               7   \\\n",
       "0        (man, 188)    (take, 146)   (call, 143)    (ahern, 134)   \n",
       "1         (dead, 1)   (arminta, 1)  (wallace, 1)       (walk, 1)   \n",
       "2         (long, 1)      (lane, 1)      (hop, 1)      (sharp, 1)   \n",
       "3          (ban, 1)    (advert, 1)   (dublin, 1)       (city, 1)   \n",
       "4    (lifeline, 14)     (telly, 1)  (version, 1)     (better, 1)   \n",
       "..              ...            ...           ...             ...   \n",
       "127        (rub, 3)  (recovery, 3)    (ahern, 2)      (party, 2)   \n",
       "128      (light, 9)     (black, 5)     (card, 5)       (make, 5)   \n",
       "129   (arsenal, 10)     (clear, 8)    (alive, 8)       (move, 7)   \n",
       "130       (back, 4)    (defeat, 4)    (fight, 4)       (keep, 3)   \n",
       "131   (victory, 21)     (seal, 15)   (claim, 14)  (champion, 11)   \n",
       "\n",
       "                8               9   ...           11            12  \\\n",
       "0      (plan, 132)      (get, 131)  ...   (set, 129)   (back, 126)   \n",
       "1         (may, 1)             NaN  ...          NaN           NaN   \n",
       "2       (chest, 1)      (shock, 1)  ...   (sight, 1)    (drive, 1)   \n",
       "3      (centre, 1)    (concern, 1)  ...  (lisbon, 1)           NaN   \n",
       "4    (throwing, 1)  (waterford, 1)  ...    (make, 1)    (sense, 1)   \n",
       "..             ...             ...  ...          ...           ...   \n",
       "127     (shade, 2)     (leader, 1)  ...  (hoping, 1)     (spec, 1)   \n",
       "128      (cold, 3)       (chip, 3)  ...     (get, 3)  (diamond, 3)   \n",
       "129   (winning, 7)     (strong, 6)  ...    (hope, 5)  (canning, 5)   \n",
       "130     (title, 3)      (singh, 2)  ...   (front, 2)    (alive, 2)   \n",
       "131    (derry, 10)       (late, 9)  ...  (finish, 7)    (first, 6)   \n",
       "\n",
       "               13           14            15             16             17  \\\n",
       "0    (court, 120)  (home, 114)   (talk, 108)  (report, 106)    (case, 105)   \n",
       "1             NaN          NaN           NaN            NaN            NaN   \n",
       "2     (ulster, 1)          NaN           NaN            NaN            NaN   \n",
       "3             NaN          NaN           NaN            NaN            NaN   \n",
       "4             NaN          NaN           NaN            NaN            NaN   \n",
       "..            ...          ...           ...            ...            ...   \n",
       "127  (promise, 1)     (new, 1)  (insight, 1)      (lady, 1)       (red, 1)   \n",
       "128    (still, 3)  (dublin, 3)      (see, 3)   (success, 3)      (side, 3)   \n",
       "129    (track, 5)   (prove, 5)     (luck, 4)      (pole, 4)  (position, 4)   \n",
       "130     (game, 2)     (say, 2)      (run, 2)     (walsh, 2)     (award, 2)   \n",
       "131      (top, 6)    (take, 5)    (point, 5)  (comeback, 5)    (snatch, 5)   \n",
       "\n",
       "                18             19              20  \n",
       "0      (year, 105)    (make, 104)      (time, 99)  \n",
       "1              NaN            NaN             NaN  \n",
       "2              NaN            NaN             NaN  \n",
       "3              NaN            NaN             NaN  \n",
       "4              NaN            NaN             NaN  \n",
       "..             ...            ...             ...  \n",
       "127     (shoot, 1)  (reappear, 1)  (tipperary, 1)  \n",
       "128     (brown, 3)      (time, 2)    (ability, 2)  \n",
       "129     (brace, 4)   (formula, 4)        (put, 4)  \n",
       "130  (sullivan, 2)     (clear, 2)    (russell, 2)  \n",
       "131     (world, 5)       (pat, 5)        (two, 4)  \n",
       "\n",
       "[132 rows x 21 columns]"
      ]
     },
     "execution_count": 311,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ireland_freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['galway', 'give', 'canning', 'senior', 'bow'],\n",
       " ['ride'],\n",
       " ['loughlin', 'put', 'frame', 'olympic', 'place'],\n",
       " ['drogheda', 'strong'],\n",
       " ['connell', 'dominant', 'form', 'munster', 'prevail'],\n",
       " ['laois', 'left', 'ruing', 'luck'],\n",
       " ['brennan', 'brace', 'keep', 'bohs', 'touch'],\n",
       " ['mcfadden', 'stop', 'arsenal', 'track'],\n",
       " ['record', 'shot', 'winning', 'margin', 'oosthuizen'],\n",
       " ['curragh', 'run', 'oxx', 'kargali'],\n",
       " ['arsenal', 'refocused', 'big', 'stage'],\n",
       " ['calzaghe', 'keep', 'cool'],\n",
       " ['galway', 'salute', 'second', 'canning'],\n",
       " ['sligo', 'dominate', 'fail', 'make', 'breakthrough'],\n",
       " ['record']]"
      ]
     },
     "execution_count": 314,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_headlines(clusters=ireland_clusters, label=128, embedding=ireland_embed, number = 15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PZG3nFTa4-18"
   },
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "notebook_a3.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
