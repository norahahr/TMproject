{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AQzYGMVl4-1n"
   },
   "source": [
    "# Text Mining -  Group Project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qRnI9EF74-1t"
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TTwyDa3M4-1v"
   },
   "source": [
    "# Preparing the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "id": "x0DzGcra4-1x"
   },
   "outputs": [],
   "source": [
    "import gensim\n",
    "import hdbscan\n",
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import re\n",
    "import string\n",
    "import umap\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "from nltk.tokenize import regexp_tokenize\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reddit News"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "WZ2OPwtS4-10"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reddit_file_clmns: Index(['time_created', 'date_created', 'up_votes', 'down_votes', 'title',\n",
      "       'over_18', 'author', 'subreddit'],\n",
      "      dtype='object')\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Headline</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Scores killed in Pakistan clashes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Japan resumes refuelling mission</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>US presses Egypt on Gaza border</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Jump-start economy: Give health care to all</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Council of Europe bashes EU&amp;UN terror blacklist</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          Headline\n",
       "0                Scores killed in Pakistan clashes\n",
       "1                 Japan resumes refuelling mission\n",
       "2                  US presses Egypt on Gaza border\n",
       "3     Jump-start economy: Give health care to all \n",
       "4  Council of Europe bashes EU&UN terror blacklist"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reddit_path = \"data/reddit_worldnews.csv\"\n",
    "reddit_file = pd.read_csv(reddit_path, encoding=\"utf-8\", encoding_errors=\"ignore\")\n",
    "print(\"reddit_file_clmns:\", reddit_file.columns)\n",
    "reddit = pd.DataFrame(reddit_file[\"title\"]).rename(columns={\"title\":\"Headline\"})\n",
    "reddit.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Irland News"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "id": "WZ2OPwtS4-10",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ireland_file_clmns: Index(['publish_date', 'headline_category', 'headline_text'], dtype='object')\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Headline</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>UUP sees possibility of voting Major out</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Pubs targeted as curbs on smoking are extended</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Papers reveal secret links with O'Neill cabinet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Domestic chaos as Italy takes EU presidency</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Learning about the star to which we owe life</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          Headline\n",
       "0         UUP sees possibility of voting Major out\n",
       "1   Pubs targeted as curbs on smoking are extended\n",
       "2  Papers reveal secret links with O'Neill cabinet\n",
       "3      Domestic chaos as Italy takes EU presidency\n",
       "4     Learning about the star to which we owe life"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ireland_path = \"data/ireland-news-headlines.csv\"\n",
    "ireland_file = pd.read_csv(ireland_path, encoding=\"utf-8\", encoding_errors=\"ignore\")\n",
    "print(\"ireland_file_clmns:\", ireland_file.columns)\n",
    "ireland_filtered_date = ireland_file[(ireland_file[\"publish_date\"] >= 20080125) & \n",
    "                                     (ireland_file[\"publish_date\"] <= 20161122)]\n",
    "ireland = pd.DataFrame(ireland_file[\"headline_text\"]).rename(columns={\"headline_text\":\"Headline\"})\n",
    "ireland.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pandas.core.frame.DataFrame"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(ireland_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pre-processing \n",
    "# Regular expression used for tokenization\n",
    "pattern = r'''(?x)    \n",
    "(?:[A-Z]\\.)+          \n",
    "|\\w+(?:-\\w+)*         \n",
    "|\\$?\\d+(?:\\.\\d+)?%?   \n",
    "|\\.\\.\\.               \n",
    "|[][.,;\"\\'?():-_`]  \n",
    "'''\n",
    "\n",
    "# Lemmatizer used \n",
    "lemmatizer = nltk.WordNetLemmatizer()\n",
    "\n",
    "def preprocessing(df):\n",
    "    \"\"\"Input: dataframe\n",
    "       Output: preprocessed dataframe\"\"\"\n",
    "    \n",
    "    # Reduce amount of data for quicker training purposes\n",
    "    # Headline = df[\"Headline\"].head(100)\n",
    "        \n",
    "    # Get the stopwords and punctuation\n",
    "    stopwords = nltk.corpus.stopwords.words('english')\n",
    "    punct = list(string.punctuation)\n",
    "    \n",
    "    # Initialize tokenized list of headlines\n",
    "    # Get list of headlines\n",
    "    headlns_lst = df[\"Headline\"].to_list()\n",
    "    \n",
    "    tokenized_lines = []\n",
    "    for headln in headlns_lst:\n",
    "        line = str(headln).strip().lower()\n",
    "        line = regexp_tokenize(line, pattern)\n",
    "        line = [tok for tok in line if tok not in stopwords and tok not in punct and tok.isalpha() and len(tok)>2]\n",
    "        tokenized_lines.append(line)\n",
    "    \n",
    "    # Initialize lemmatized list of headlines\n",
    "    pp_df = pd.DataFrame(columns = [\"Headline\"])\n",
    "    \n",
    "    lemmatized_lines = [[lemmatizer.lemmatize(token) for token in headln] for headln in tokenized_lines]\n",
    "    line_df = pd.DataFrame({\"Headline\": lemmatized_lines})\n",
    "    pp_df = pp_df.append(line_df, ignore_index=True)\n",
    " \n",
    "    return pp_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reddit News"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Headline</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[score, killed, pakistan, clash]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[japan, resume, refuelling, mission]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[press, egypt, gaza, border]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[economy, give, health, care]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[council, europe, bash, terror, blacklist]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>509231</th>\n",
       "      <td>[heil, trump, donald, trump, white, nationalis...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>509232</th>\n",
       "      <td>[people, speculating, could, madeleine, mccann]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>509233</th>\n",
       "      <td>[professor, receives, arab, researcher, award]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>509234</th>\n",
       "      <td>[nigel, farage, attack, response, trump, ambas...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>509235</th>\n",
       "      <td>[palestinian, wielding, knife, shot, dead, wes...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>509236 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 Headline\n",
       "0                        [score, killed, pakistan, clash]\n",
       "1                    [japan, resume, refuelling, mission]\n",
       "2                            [press, egypt, gaza, border]\n",
       "3                           [economy, give, health, care]\n",
       "4              [council, europe, bash, terror, blacklist]\n",
       "...                                                   ...\n",
       "509231  [heil, trump, donald, trump, white, nationalis...\n",
       "509232    [people, speculating, could, madeleine, mccann]\n",
       "509233     [professor, receives, arab, researcher, award]\n",
       "509234  [nigel, farage, attack, response, trump, ambas...\n",
       "509235  [palestinian, wielding, knife, shot, dead, wes...\n",
       "\n",
       "[509236 rows x 1 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reddit_pp = preprocessing(reddit)\n",
    "reddit_pp\n",
    "t = timeit.timeit(lambda:reddit_pp)\n",
    "print(t)\n",
    "print(reddit_pp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Irland News"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Headline</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[uup, see, possibility, voting, major]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[pub, targeted, curb, smoking, extended]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[paper, reveal, secret, link, neill, cabinet]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[domestic, chaos, italy, take, presidency]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[learning, star, owe, life]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1611490</th>\n",
       "      <td>[reserve, member, defence, force, allowed, ser...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1611491</th>\n",
       "      <td>[maureen, dowd, joe, biden, crazy, irish, plan...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1611492</th>\n",
       "      <td>[andy, murray, roll, back, year, centre, court]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1611493</th>\n",
       "      <td>[delta, variant, could, significant, damage, p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1611494</th>\n",
       "      <td>[gordon, brown, got, give, people, message, hope]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1611495 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  Headline\n",
       "0                   [uup, see, possibility, voting, major]\n",
       "1                 [pub, targeted, curb, smoking, extended]\n",
       "2            [paper, reveal, secret, link, neill, cabinet]\n",
       "3               [domestic, chaos, italy, take, presidency]\n",
       "4                              [learning, star, owe, life]\n",
       "...                                                    ...\n",
       "1611490  [reserve, member, defence, force, allowed, ser...\n",
       "1611491  [maureen, dowd, joe, biden, crazy, irish, plan...\n",
       "1611492    [andy, murray, roll, back, year, centre, court]\n",
       "1611493  [delta, variant, could, significant, damage, p...\n",
       "1611494  [gordon, brown, got, give, people, message, hope]\n",
       "\n",
       "[1611495 rows x 1 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ireland_pp = preprocessing(ireland)\n",
    "ireland_pp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Need to use pre-trained embeddings since headlines not enough\n",
    "# Should we cut out randomly some data of the ireland dataset so that we have equal amount or do we want to normalize in the\n",
    "# end the amount of headlines per theme for the amount of data ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YwLrjSBV4-18"
   },
   "source": [
    "# Sentence Embeddings\n",
    "\n",
    "We are sentence embeddings by averaging the pre-trained word embeddings from the GoogleNews vectors. These embeddings can be found at https://drive.google.com/file/d/0B7XkCwpI5KDYNlNUTTlSS21pQmM/edit?resourcekey=0-wjGZdNAUop6WykTtMip30g."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The goal is to be able to read the GoogleNews from its URL but haven't found a simple way to read a \n",
    "# bin.gz file from URL yet. \n",
    "model = gensim.models.KeyedVectors.load_word2vec_format('data/GoogleNews-vectors-negative300.bin.gz', binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_vector(word2vec_model, doc):\n",
    "    \"\"\"Calculate the mean vector according to a word2vec model for one document/headline\"\"\"\n",
    "    # remove out-of-vocabulary words\n",
    "    doc = [word for word in doc if word in word2vec_model]\n",
    "    if len(doc) >= 1:\n",
    "        return sum(word2vec_model[doc])/len(doc)\n",
    "    else:\n",
    "        return []\n",
    "\n",
    "def doc_embeddings(dataset):\n",
    "    \"\"\"Calculate the mean vector for all documents in a dataset.\n",
    "    Outputs a dataframe with document and 300 dim vector representation of it.\"\"\"\n",
    "    embeddings_df = pd.DataFrame()\n",
    "\n",
    "    for doc in dataset[\"Headline\"]:\n",
    "        vec = mean_vector(model, doc)\n",
    "        if len(vec) > 0:\n",
    "            vec_df = pd.Series(vec)\n",
    "            doc_df = pd.Series([doc]).append(vec_df, ignore_index = True)\n",
    "            embeddings_df = embeddings_df.append(doc_df, ignore_index=True) \n",
    "    \n",
    "    return embeddings_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use all the data, not just first 1000 headlines when finalizing process.\n",
    "reddit_embed = doc_embeddings(reddit_pp)\n",
    "reddit_embed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>291</th>\n",
       "      <th>292</th>\n",
       "      <th>293</th>\n",
       "      <th>294</th>\n",
       "      <th>295</th>\n",
       "      <th>296</th>\n",
       "      <th>297</th>\n",
       "      <th>298</th>\n",
       "      <th>299</th>\n",
       "      <th>300</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[uup, see, possibility, voting, major]</td>\n",
       "      <td>-0.019440</td>\n",
       "      <td>0.096291</td>\n",
       "      <td>0.004242</td>\n",
       "      <td>0.111938</td>\n",
       "      <td>-0.122192</td>\n",
       "      <td>0.011169</td>\n",
       "      <td>0.063950</td>\n",
       "      <td>-0.051147</td>\n",
       "      <td>0.126831</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.105103</td>\n",
       "      <td>-0.063385</td>\n",
       "      <td>-0.040253</td>\n",
       "      <td>-0.077667</td>\n",
       "      <td>0.060120</td>\n",
       "      <td>-0.015381</td>\n",
       "      <td>-0.030247</td>\n",
       "      <td>-0.026241</td>\n",
       "      <td>0.083755</td>\n",
       "      <td>-0.034698</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[pub, targeted, curb, smoking, extended]</td>\n",
       "      <td>-0.083624</td>\n",
       "      <td>0.037817</td>\n",
       "      <td>-0.006555</td>\n",
       "      <td>0.198828</td>\n",
       "      <td>-0.109766</td>\n",
       "      <td>-0.000049</td>\n",
       "      <td>-0.002713</td>\n",
       "      <td>-0.003613</td>\n",
       "      <td>0.247559</td>\n",
       "      <td>...</td>\n",
       "      <td>0.015234</td>\n",
       "      <td>0.042723</td>\n",
       "      <td>-0.034412</td>\n",
       "      <td>0.052075</td>\n",
       "      <td>-0.091187</td>\n",
       "      <td>0.059448</td>\n",
       "      <td>0.062207</td>\n",
       "      <td>0.046216</td>\n",
       "      <td>0.077539</td>\n",
       "      <td>-0.031396</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[paper, reveal, secret, link, neill, cabinet]</td>\n",
       "      <td>-0.109766</td>\n",
       "      <td>0.024805</td>\n",
       "      <td>-0.097681</td>\n",
       "      <td>-0.043976</td>\n",
       "      <td>-0.031348</td>\n",
       "      <td>0.067725</td>\n",
       "      <td>0.056592</td>\n",
       "      <td>-0.013770</td>\n",
       "      <td>0.245703</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.072803</td>\n",
       "      <td>0.006732</td>\n",
       "      <td>-0.129858</td>\n",
       "      <td>0.046387</td>\n",
       "      <td>-0.097412</td>\n",
       "      <td>-0.167065</td>\n",
       "      <td>0.065753</td>\n",
       "      <td>-0.116599</td>\n",
       "      <td>-0.008594</td>\n",
       "      <td>0.048206</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[domestic, chaos, italy, take, presidency]</td>\n",
       "      <td>-0.069678</td>\n",
       "      <td>0.009131</td>\n",
       "      <td>0.003613</td>\n",
       "      <td>0.110767</td>\n",
       "      <td>-0.011865</td>\n",
       "      <td>-0.110742</td>\n",
       "      <td>0.000977</td>\n",
       "      <td>-0.161548</td>\n",
       "      <td>0.070337</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.104053</td>\n",
       "      <td>-0.004004</td>\n",
       "      <td>-0.069653</td>\n",
       "      <td>0.019873</td>\n",
       "      <td>-0.081329</td>\n",
       "      <td>0.116309</td>\n",
       "      <td>-0.025882</td>\n",
       "      <td>-0.066553</td>\n",
       "      <td>0.028613</td>\n",
       "      <td>0.098199</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[learning, star, owe, life]</td>\n",
       "      <td>0.045166</td>\n",
       "      <td>0.118469</td>\n",
       "      <td>0.081787</td>\n",
       "      <td>0.070648</td>\n",
       "      <td>0.093658</td>\n",
       "      <td>0.063385</td>\n",
       "      <td>0.142822</td>\n",
       "      <td>-0.122314</td>\n",
       "      <td>0.066040</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.059326</td>\n",
       "      <td>0.094330</td>\n",
       "      <td>-0.176895</td>\n",
       "      <td>-0.117706</td>\n",
       "      <td>-0.069595</td>\n",
       "      <td>-0.030823</td>\n",
       "      <td>0.028152</td>\n",
       "      <td>0.034851</td>\n",
       "      <td>-0.002686</td>\n",
       "      <td>0.005997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>[decisive, loss]</td>\n",
       "      <td>0.072021</td>\n",
       "      <td>0.074402</td>\n",
       "      <td>0.037495</td>\n",
       "      <td>-0.089233</td>\n",
       "      <td>0.029663</td>\n",
       "      <td>-0.036621</td>\n",
       "      <td>-0.067627</td>\n",
       "      <td>-0.306641</td>\n",
       "      <td>0.217285</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001862</td>\n",
       "      <td>-0.014404</td>\n",
       "      <td>0.019531</td>\n",
       "      <td>-0.077393</td>\n",
       "      <td>-0.056885</td>\n",
       "      <td>0.110840</td>\n",
       "      <td>-0.101562</td>\n",
       "      <td>-0.075928</td>\n",
       "      <td>-0.063599</td>\n",
       "      <td>0.069092</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>[meaty, food]</td>\n",
       "      <td>-0.113281</td>\n",
       "      <td>0.130859</td>\n",
       "      <td>-0.164062</td>\n",
       "      <td>0.199097</td>\n",
       "      <td>-0.008545</td>\n",
       "      <td>0.028320</td>\n",
       "      <td>0.063965</td>\n",
       "      <td>-0.101440</td>\n",
       "      <td>-0.188965</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.204590</td>\n",
       "      <td>-0.239502</td>\n",
       "      <td>-0.059875</td>\n",
       "      <td>0.279297</td>\n",
       "      <td>0.058777</td>\n",
       "      <td>0.008423</td>\n",
       "      <td>0.044922</td>\n",
       "      <td>0.039307</td>\n",
       "      <td>0.031982</td>\n",
       "      <td>0.100235</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>[terminal, opportunism]</td>\n",
       "      <td>0.250977</td>\n",
       "      <td>0.082642</td>\n",
       "      <td>-0.138916</td>\n",
       "      <td>0.208008</td>\n",
       "      <td>-0.098938</td>\n",
       "      <td>-0.235352</td>\n",
       "      <td>-0.096924</td>\n",
       "      <td>-0.029846</td>\n",
       "      <td>0.259766</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.155945</td>\n",
       "      <td>-0.179688</td>\n",
       "      <td>-0.023926</td>\n",
       "      <td>0.000244</td>\n",
       "      <td>-0.083679</td>\n",
       "      <td>-0.063477</td>\n",
       "      <td>0.019531</td>\n",
       "      <td>0.001221</td>\n",
       "      <td>-0.126709</td>\n",
       "      <td>0.212280</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>[surprised, scribe]</td>\n",
       "      <td>0.082886</td>\n",
       "      <td>0.087036</td>\n",
       "      <td>0.039307</td>\n",
       "      <td>-0.024536</td>\n",
       "      <td>0.002625</td>\n",
       "      <td>0.089111</td>\n",
       "      <td>0.089844</td>\n",
       "      <td>0.116455</td>\n",
       "      <td>0.133461</td>\n",
       "      <td>...</td>\n",
       "      <td>0.100708</td>\n",
       "      <td>-0.163574</td>\n",
       "      <td>0.015137</td>\n",
       "      <td>0.006470</td>\n",
       "      <td>-0.064697</td>\n",
       "      <td>-0.154785</td>\n",
       "      <td>-0.206543</td>\n",
       "      <td>-0.067993</td>\n",
       "      <td>-0.052246</td>\n",
       "      <td>0.055420</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>[mountjoy, fire]</td>\n",
       "      <td>0.355469</td>\n",
       "      <td>0.183594</td>\n",
       "      <td>0.149414</td>\n",
       "      <td>-0.093750</td>\n",
       "      <td>0.178711</td>\n",
       "      <td>-0.083984</td>\n",
       "      <td>-0.063965</td>\n",
       "      <td>-0.257812</td>\n",
       "      <td>-0.117188</td>\n",
       "      <td>...</td>\n",
       "      <td>0.215820</td>\n",
       "      <td>-0.261719</td>\n",
       "      <td>-0.269531</td>\n",
       "      <td>0.116211</td>\n",
       "      <td>0.021606</td>\n",
       "      <td>-0.060791</td>\n",
       "      <td>-0.189453</td>\n",
       "      <td>0.049805</td>\n",
       "      <td>-0.222656</td>\n",
       "      <td>0.004059</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>98 rows × 301 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              0         1         2    \\\n",
       "0          [uup, see, possibility, voting, major] -0.019440  0.096291   \n",
       "1        [pub, targeted, curb, smoking, extended] -0.083624  0.037817   \n",
       "2   [paper, reveal, secret, link, neill, cabinet] -0.109766  0.024805   \n",
       "3      [domestic, chaos, italy, take, presidency] -0.069678  0.009131   \n",
       "4                     [learning, star, owe, life]  0.045166  0.118469   \n",
       "..                                            ...       ...       ...   \n",
       "93                               [decisive, loss]  0.072021  0.074402   \n",
       "94                                  [meaty, food] -0.113281  0.130859   \n",
       "95                        [terminal, opportunism]  0.250977  0.082642   \n",
       "96                            [surprised, scribe]  0.082886  0.087036   \n",
       "97                               [mountjoy, fire]  0.355469  0.183594   \n",
       "\n",
       "         3         4         5         6         7         8         9    ...  \\\n",
       "0   0.004242  0.111938 -0.122192  0.011169  0.063950 -0.051147  0.126831  ...   \n",
       "1  -0.006555  0.198828 -0.109766 -0.000049 -0.002713 -0.003613  0.247559  ...   \n",
       "2  -0.097681 -0.043976 -0.031348  0.067725  0.056592 -0.013770  0.245703  ...   \n",
       "3   0.003613  0.110767 -0.011865 -0.110742  0.000977 -0.161548  0.070337  ...   \n",
       "4   0.081787  0.070648  0.093658  0.063385  0.142822 -0.122314  0.066040  ...   \n",
       "..       ...       ...       ...       ...       ...       ...       ...  ...   \n",
       "93  0.037495 -0.089233  0.029663 -0.036621 -0.067627 -0.306641  0.217285  ...   \n",
       "94 -0.164062  0.199097 -0.008545  0.028320  0.063965 -0.101440 -0.188965  ...   \n",
       "95 -0.138916  0.208008 -0.098938 -0.235352 -0.096924 -0.029846  0.259766  ...   \n",
       "96  0.039307 -0.024536  0.002625  0.089111  0.089844  0.116455  0.133461  ...   \n",
       "97  0.149414 -0.093750  0.178711 -0.083984 -0.063965 -0.257812 -0.117188  ...   \n",
       "\n",
       "         291       292       293       294       295       296       297  \\\n",
       "0  -0.105103 -0.063385 -0.040253 -0.077667  0.060120 -0.015381 -0.030247   \n",
       "1   0.015234  0.042723 -0.034412  0.052075 -0.091187  0.059448  0.062207   \n",
       "2  -0.072803  0.006732 -0.129858  0.046387 -0.097412 -0.167065  0.065753   \n",
       "3  -0.104053 -0.004004 -0.069653  0.019873 -0.081329  0.116309 -0.025882   \n",
       "4  -0.059326  0.094330 -0.176895 -0.117706 -0.069595 -0.030823  0.028152   \n",
       "..       ...       ...       ...       ...       ...       ...       ...   \n",
       "93  0.001862 -0.014404  0.019531 -0.077393 -0.056885  0.110840 -0.101562   \n",
       "94 -0.204590 -0.239502 -0.059875  0.279297  0.058777  0.008423  0.044922   \n",
       "95 -0.155945 -0.179688 -0.023926  0.000244 -0.083679 -0.063477  0.019531   \n",
       "96  0.100708 -0.163574  0.015137  0.006470 -0.064697 -0.154785 -0.206543   \n",
       "97  0.215820 -0.261719 -0.269531  0.116211  0.021606 -0.060791 -0.189453   \n",
       "\n",
       "         298       299       300  \n",
       "0  -0.026241  0.083755 -0.034698  \n",
       "1   0.046216  0.077539 -0.031396  \n",
       "2  -0.116599 -0.008594  0.048206  \n",
       "3  -0.066553  0.028613  0.098199  \n",
       "4   0.034851 -0.002686  0.005997  \n",
       "..       ...       ...       ...  \n",
       "93 -0.075928 -0.063599  0.069092  \n",
       "94  0.039307  0.031982  0.100235  \n",
       "95  0.001221 -0.126709  0.212280  \n",
       "96 -0.067993 -0.052246  0.055420  \n",
       "97  0.049805 -0.222656  0.004059  \n",
       "\n",
       "[98 rows x 301 columns]"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ireland_embed = doc_embeddings(ireland_pp)\n",
    "ireland_embed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.0194397 ,  0.09629059,  0.00424194, ..., -0.0262413 ,\n",
       "         0.08375549, -0.03469849],\n",
       "       [-0.08362427,  0.03781738, -0.00655518, ...,  0.04621582,\n",
       "         0.07753906, -0.03139649],\n",
       "       [-0.10976563,  0.02480469, -0.09768067, ..., -0.11659851,\n",
       "        -0.00859375,  0.04820557],\n",
       "       ...,\n",
       "       [ 0.25097656,  0.0826416 , -0.13891602, ...,  0.0012207 ,\n",
       "        -0.12670898,  0.21228027],\n",
       "       [ 0.08288574,  0.08703613,  0.03930664, ..., -0.06799316,\n",
       "        -0.05224609,  0.05541992],\n",
       "       [ 0.35546875,  0.18359375,  0.14941406, ...,  0.04980469,\n",
       "        -0.22265625,  0.00405884]])"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ireland_embed.loc[:, 1:].to_numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eeR26J-N4-15"
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_clusters(sentence_embeddings,\n",
    "                      n_neighbors,\n",
    "                      n_components, \n",
    "                      min_cluster_size,\n",
    "                      random_state = None):\n",
    "    \"\"\"\n",
    "    Generate HDBSCAN cluster object after reducing embedding dimensionality with UMAP\n",
    "    \"\"\"\n",
    "    \n",
    "    umap_embeddings = umap.UMAP(n_neighbors=n_neighbors, \n",
    "                                n_components=n_components, \n",
    "                                metric='cosine', \n",
    "                                random_state=random_state).fit_transform(sentence_embeddings.loc[:, 1:].to_numpy())\n",
    "\n",
    "    clusters = hdbscan.HDBSCAN(min_cluster_size = min_cluster_size,\n",
    "                               metric='euclidean', \n",
    "                               cluster_selection_method='eom').fit(umap_embeddings)\n",
    "    \n",
    "\n",
    "    return clusters\n",
    "\n",
    "\n",
    "def score_clusters(clusters, prob_threshold = 0.05):\n",
    "    \"\"\"\n",
    "    Returns the label count and cost of a given cluster supplied from running hdbscan\n",
    "    \"\"\"\n",
    "    \n",
    "    cluster_labels = clusters.labels_\n",
    "    label_count = len(np.unique(cluster_labels))\n",
    "    total_num = len(clusters.labels_)\n",
    "    cost = (np.count_nonzero(clusters.probabilities_ < prob_threshold)/total_num)\n",
    "    \n",
    "    return label_count, cost\n",
    "\n",
    "\n",
    "def random_search(embeddings, space, num_evals):\n",
    "    \"\"\"\n",
    "    Randomly search hyperparameter space and limited number of times \n",
    "    and return a summary of the results\n",
    "    \"\"\"\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    for i in range(num_evals):\n",
    "        n_neighbors = random.choice(space['n_neighbors'])\n",
    "        n_components = random.choice(space['n_components'])\n",
    "        min_cluster_size = random.choice(space['min_cluster_size'])\n",
    "        \n",
    "        clusters = generate_clusters(embeddings, \n",
    "                                     n_neighbors = n_neighbors, \n",
    "                                     n_components = n_components, \n",
    "                                     min_cluster_size = min_cluster_size, \n",
    "                                     random_state = 42)\n",
    "    \n",
    "        label_count, cost = score_clusters(clusters, prob_threshold = 0.05)\n",
    "                \n",
    "        results.append([i, n_neighbors, n_components, min_cluster_size, \n",
    "                        label_count, cost])\n",
    "    \n",
    "    result_df = pd.DataFrame(results, columns=['run_id', 'n_neighbors', 'n_components', \n",
    "                                               'min_cluster_size', 'label_count', 'cost'])\n",
    "    \n",
    "    return result_df.sort_values(by='cost')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>run_id</th>\n",
       "      <th>n_neighbors</th>\n",
       "      <th>n_components</th>\n",
       "      <th>min_cluster_size</th>\n",
       "      <th>label_count</th>\n",
       "      <th>cost</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>78</td>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "      <td>9</td>\n",
       "      <td>2</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>45</td>\n",
       "      <td>7</td>\n",
       "      <td>4</td>\n",
       "      <td>10</td>\n",
       "      <td>3</td>\n",
       "      <td>0.001001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>22</td>\n",
       "      <td>7</td>\n",
       "      <td>4</td>\n",
       "      <td>11</td>\n",
       "      <td>3</td>\n",
       "      <td>0.001001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>16</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>10</td>\n",
       "      <td>3</td>\n",
       "      <td>0.003003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>83</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "      <td>3</td>\n",
       "      <td>0.008008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>35</td>\n",
       "      <td>9</td>\n",
       "      <td>4</td>\n",
       "      <td>13</td>\n",
       "      <td>15</td>\n",
       "      <td>0.574575</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>63</td>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "      <td>14</td>\n",
       "      <td>14</td>\n",
       "      <td>0.583584</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>10</td>\n",
       "      <td>11</td>\n",
       "      <td>3</td>\n",
       "      <td>13</td>\n",
       "      <td>15</td>\n",
       "      <td>0.605606</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>57</td>\n",
       "      <td>13</td>\n",
       "      <td>3</td>\n",
       "      <td>14</td>\n",
       "      <td>12</td>\n",
       "      <td>0.627628</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>34</td>\n",
       "      <td>9</td>\n",
       "      <td>4</td>\n",
       "      <td>14</td>\n",
       "      <td>12</td>\n",
       "      <td>0.634635</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    run_id  n_neighbors  n_components  min_cluster_size  label_count      cost\n",
       "78      78            7             3                 9            2  0.000000\n",
       "45      45            7             4                10            3  0.001001\n",
       "22      22            7             4                11            3  0.001001\n",
       "16      16            7             2                10            3  0.003003\n",
       "83      83            6             4                 8            3  0.008008\n",
       "..     ...          ...           ...               ...          ...       ...\n",
       "35      35            9             4                13           15  0.574575\n",
       "63      63            7             3                14           14  0.583584\n",
       "10      10           11             3                13           15  0.605606\n",
       "57      57           13             3                14           12  0.627628\n",
       "34      34            9             4                14           12  0.634635\n",
       "\n",
       "[100 rows x 6 columns]"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "space = {\"n_neighbors\": range(5,15),\n",
    "        \"n_components\": range(2,7),\n",
    "        \"min_cluster_size\": range(2,15),\n",
    "        \"random_state\": 42}\n",
    "\n",
    "random_search(embeddings=reddit_embed, space=space, num_evals= 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "reddit_clusters = generate_clusters(sentence_embeddings= reddit_embed, n_neighbors= 3, n_components=3, min_cluster_size=3, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JyDE26-i4-15"
   },
   "source": [
    "## Splitting data into a train and a test set \n",
    "80% for training and 20% for testing.\n",
    "Data is shuffled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "reddit_df = shuffle(reddit_pp, random_state=42)\n",
    "ireland_df = shuffle(ireland_pp, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "id": "9vWDVkRB4-16"
   },
   "outputs": [],
   "source": [
    "# Train Test Split\n",
    "train_reddit, test_reddit = train_test_split(reddit_df, test_size=0.20, random_state=42)\n",
    "train_ireland, test_ireland = train_test_split(ireland_df, test_size=0.20, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PZG3nFTa4-18"
   },
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "notebook_a3.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
